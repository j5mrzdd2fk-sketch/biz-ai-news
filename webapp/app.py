#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ« - Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""

import os
import sys
import traceback
import hashlib
import urllib.parse
import re
from flask import Flask, render_template, jsonify, request, Response, abort, make_response
from datetime import datetime

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import gspread
from google.oauth2.credentials import Credentials

# ãƒ­ã‚°è¨­å®š
from logger_config import get_webapp_logger, log_exception

app = Flask(__name__)

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è‡ªå‹•ãƒªãƒ­ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–ï¼ˆé–‹ç™ºæ™‚ï¼‰
app.config['TEMPLATES_AUTO_RELOAD'] = True

# ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–
logger = get_webapp_logger()

# è¨­å®š
SPREADSHEET_NAME = "AIãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ï¼ˆãƒãƒ«ãƒã‚µã‚¤ãƒˆï¼‰"

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—ï¼ˆRenderç”¨ï¼‰
# ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€
GOOGLE_SHEETS_TOKEN = os.getenv('GOOGLE_SHEETS_TOKEN')
GOOGLE_SHEETS_CREDENTIALS = os.getenv('GOOGLE_SHEETS_CREDENTIALS')

# ãƒˆãƒ¼ã‚¯ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨ï¼‰
TOKEN_FILE = os.path.join(os.path.dirname(os.path.dirname(__file__)), "token.json")
CREDENTIALS_FILE = os.path.join(os.path.dirname(os.path.dirname(__file__)), "credentials.json")

SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive.file']


def generate_article_id(title, date, source):
    """è¨˜äº‹ã®ä¸€æ„ã®IDã‚’ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€æ—¥ä»˜ã€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï¼‰"""
    # ã‚¿ã‚¤ãƒˆãƒ«ã€æ—¥ä»˜ã€ã‚½ãƒ¼ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒãƒƒã‚·ãƒ¥åŒ–
    combined = f"{title}|{date}|{source}"
    # SHA256ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆã—ã€æœ€åˆã®16æ–‡å­—ã‚’ä½¿ç”¨
    article_id = hashlib.sha256(combined.encode('utf-8')).hexdigest()[:16]
    return article_id


def parse_date_to_datetime(date_str):
    """æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ï¼ˆã‚½ãƒ¼ãƒˆç”¨ï¼‰"""
    if not date_str:
        return datetime(1900, 1, 1)  # æ—¥ä»˜ãŒãªã„å ´åˆã¯éå¸¸ã«å¤ã„æ—¥ä»˜ã¨ã—ã¦æ‰±ã†
    
    # æ§˜ã€…ãªæ—¥ä»˜å½¢å¼ã«å¯¾å¿œ
    date_formats = [
        "%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†",  # 2025å¹´12æœˆ9æ—¥ 15æ™‚30åˆ†
        "%Yå¹´%mæœˆ%dæ—¥",            # 2025å¹´12æœˆ9æ—¥
        "%Y-%m-%d %H:%M:%S",       # 2025-12-09 15:30:00
        "%Y-%m-%d",                # 2025-12-09
        "%Y/%m/%d %H:%M:%S",       # 2025/12/09 15:30:00
        "%Y/%m/%d",                # 2025/12/09
    ]
    
    # æ™‚åˆ»éƒ¨åˆ†ã‚’åˆ†é›¢
    if "æ™‚" in date_str and "åˆ†" in date_str:
        # 2025å¹´12æœˆ9æ—¥ 15æ™‚30åˆ† ã®å½¢å¼
        try:
            # æ—¥ä»˜éƒ¨åˆ†ã¨æ™‚åˆ»éƒ¨åˆ†ã‚’åˆ†é›¢
            if " " in date_str:
                date_part, time_part = date_str.split(" ", 1)
            else:
                date_part = date_str
                time_part = ""
            
            # æ™‚åˆ»ã‚’æŠ½å‡º
            hour = 0
            minute = 0
            if time_part:
                hour_match = re.search(r'(\d+)æ™‚', time_part)
                minute_match = re.search(r'(\d+)åˆ†', time_part)
                if hour_match:
                    hour = int(hour_match.group(1))
                if minute_match:
                    minute = int(minute_match.group(1))
            
            # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
            date_only = re.sub(r'\d+æ™‚\d+åˆ†', '', date_part).strip()
            for fmt in ["%Yå¹´%mæœˆ%dæ—¥", "%Y-%m-%d", "%Y/%m/%d"]:
                try:
                    parsed_date = datetime.strptime(date_only, fmt)
                    return parsed_date.replace(hour=hour, minute=minute, second=0, microsecond=0)
                except ValueError:
                    continue
        except Exception:
            pass
    
    # é€šå¸¸ã®æ—¥ä»˜å½¢å¼ã‚’è©¦ã™
    for fmt in date_formats:
        try:
            return datetime.strptime(date_str, fmt)
        except ValueError:
            continue
    
    # æ­£è¦è¡¨ç¾ã§æŠ½å‡ºã‚’è©¦ã¿ã‚‹
    date_match = re.search(r'(\d{4})[/å¹´-](\d{1,2})[/æœˆ-](\d{1,2})', date_str)
    if date_match:
        year, month, day = date_match.groups()
        try:
            return datetime(int(year), int(month), int(day))
        except ValueError:
            pass
    
    # ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯éå¸¸ã«å¤ã„æ—¥ä»˜ã¨ã—ã¦æ‰±ã†
    return datetime(1900, 1, 1)


def convert_to_iso8601(date_str):
    """æ—¥ä»˜æ–‡å­—åˆ—ã‚’ISO 8601å½¢å¼ã«å¤‰æ›"""
    if not date_str:
        return None
    
    # parse_date_to_datetimeã‚’ä½¿ã£ã¦å¤‰æ›
    parsed_date = parse_date_to_datetime(date_str)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ—¥ä»˜ã®å ´åˆã¯Noneã‚’è¿”ã™
    if parsed_date == datetime(1900, 1, 1):
        return None
    
    # ISO 8601å½¢å¼ã«å¤‰æ›ï¼ˆJST: UTC+9ï¼‰
    return parsed_date.strftime("%Y-%m-%dT%H:%M:%S+09:00")


def get_sheets_client():
    """Google Sheets ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—"""
    try:
        import json
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—ï¼ˆRenderç”¨ï¼‰
        if GOOGLE_SHEETS_TOKEN:
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èª­ã¿è¾¼ã‚€
            token_data = json.loads(GOOGLE_SHEETS_TOKEN)
            creds = Credentials.from_authorized_user_info(token_data, SCOPES)
            logger.info("ç’°å¢ƒå¤‰æ•°ã‹ã‚‰Googleèªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        elif os.path.exists(TOKEN_FILE):
            # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ: ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€
            creds = Credentials.from_authorized_user_file(TOKEN_FILE, SCOPES)
            logger.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Googleèªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        else:
            raise FileNotFoundError("Googleèªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°GOOGLE_SHEETS_TOKENã¾ãŸã¯token.jsonãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚")
        
        return gspread.authorize(creds)
    except FileNotFoundError as e:
        log_exception(logger, e, "Google Sheetsèªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        raise
    except json.JSONDecodeError as e:
        log_exception(logger, e, "Googleèªè¨¼æƒ…å ±ã®JSONè§£æã‚¨ãƒ©ãƒ¼")
        raise ValueError("GOOGLE_SHEETS_TOKENã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“") from e
    except Exception as e:
        log_exception(logger, e, "Google Sheetsèªè¨¼ã‚¨ãƒ©ãƒ¼")
        raise


# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
_news_cache = None
_cache_timestamp = None
CACHE_DURATION = 60  # 1åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆçŸ­ç¸®ã—ã¦æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’åæ˜ ã—ã‚„ã™ãï¼‰

def get_all_news(use_cache=True):
    """å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰"""
    global _news_cache, _cache_timestamp
    
    import time
    current_time = time.time()
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ãªå ´åˆã¯è¿”ã™
    if use_cache and _news_cache is not None and _cache_timestamp is not None:
        if current_time - _cache_timestamp < CACHE_DURATION:
            return _news_cache
    
    try:
        client = get_sheets_client()
        spreadsheet = client.open(SPREADSHEET_NAME)
        logger.debug("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ãã¾ã—ãŸ")
    except Exception as e:
        log_exception(logger, e, "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ©ãƒ¼")
        raise
    
    all_news = []
    
    # ãƒãƒƒãƒã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ï¼‰
    try:
        for worksheet in spreadsheet.worksheets():
            category = worksheet.title
            try:
                # get_all_values()ã¯ä¸€åº¦ã«å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆåŠ¹ç‡çš„ï¼‰
                rows = worksheet.get_all_values()
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
                for row in rows[1:]:
                    if len(row) >= 7:
                        # ã‚¹ã‚³ã‚¢ã‚’æ•°å€¤ã«å¤‰æ›
                        score_str = row[5] if len(row) > 5 else ""
                        score = score_str.count("â­")
                        
                        # URLã‚’å–å¾—ï¼ˆIåˆ—ã«å®ŸURLãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°Googleæ¤œç´¢ãƒªãƒ³ã‚¯ï¼‰
                        url = ""
                        if len(row) > 8 and row[8].startswith("http"):
                            url = row[8]
                        elif len(row) > 7 and row[7].startswith("http"):
                            url = row[7]
                        else:
                            # Googleæ¤œç´¢ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ
                            title = row[2]
                            url = f"https://www.google.com/search?q={urllib.parse.quote(title)}"
                        
                        # ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—ï¼ˆJåˆ—ã«è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ã‚·ãƒ¼ãƒˆåï¼‰
                        article_category = category
                        if len(row) > 9 and row[9]:
                            article_category = row[9]  # Jåˆ—ã®ã‚«ãƒ†ã‚´ãƒªæƒ…å ±
                        
                        # è¨˜äº‹IDã‚’ç”Ÿæˆ
                        article_id = generate_article_id(row[2], row[3], row[1])
                        
                        news_item = {
                            "id": article_id,
                            "no": row[0],
                            "source": row[1],
                            "title": row[2],
                            "date": row[3],
                            "tags": row[4],
                            "score": score,
                            "score_display": score_str,
                            "summary": row[6],
                            "url": url,
                            "category": article_category  # è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªã®å ´åˆã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š
                        }
                        all_news.append(news_item)
            except Exception as e:
                log_exception(logger, e, f"ã‚·ãƒ¼ãƒˆã€Œ{category}ã€ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼")
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ä»–ã®ã‚·ãƒ¼ãƒˆã®å‡¦ç†ã¯ç¶šè¡Œ
                continue
        
        # æ—¥ä»˜é †ï¼ˆæ–°ã—ã„é †ï¼‰ã€åŒæ—¥å†…ã¯ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        # æ—¥ä»˜ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã—ã¦ã‹ã‚‰ã‚½ãƒ¼ãƒˆ
        all_news.sort(key=lambda x: (parse_date_to_datetime(x["date"]), x["score"]), reverse=True)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        _news_cache = all_news
        _cache_timestamp = current_time
        
        logger.info(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ: {len(all_news)}ä»¶")
        return all_news
    except Exception as e:
        log_exception(logger, e, "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼")
        raise


@app.route('/')
def index():
    """ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸"""
    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚Œã°å¼·åˆ¶æ›´æ–°
        force_refresh = request.args.get('refresh', '').lower() == 'true'
        if force_refresh:
            logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å¼·åˆ¶æ›´æ–°")
        news_list = get_all_news(use_cache=not force_refresh)
    except Exception as e:
        log_exception(logger, e, "ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸è¡¨ç¤ºã‚¨ãƒ©ãƒ¼")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        news_list = []
        logger.error("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™ã€‚")
    
    # ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã‚«ãƒ†ã‚´ãƒªã®å ´åˆã¯åˆ†å‰²ï¼‰
    categories_set = set()
    for n in news_list:
        for cat in [c.strip() for c in n["category"].split(",")]:
            categories_set.add(cat)
    
    # ã‚«ãƒ†ã‚´ãƒªã®è¡¨ç¤ºé †åºã‚’å›ºå®šï¼ˆå„ªå…ˆé †ä½é †ã€ã€Œãã®ä»–ã€ã‚’æœ€å¾Œã«ï¼‰
    # ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ãªãã¦ã‚‚ã€å›ºå®šã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆã‚’è¡¨ç¤º
    category_order = [
        "ä¼æ¥­åŠ¹ç‡åŒ–",
        "DXãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–",
        "ä¼æ¥­å°å…¥",
        "AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼"
    ]
    
    # å›ºå®šã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆã‚’å¸¸ã«è¡¨ç¤ºï¼ˆãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ãªã„å ´åˆã§ã‚‚ï¼‰
    categories = category_order.copy()
    # å›ºå®šé †åºã«ãªã„ã‚«ãƒ†ã‚´ãƒªã‚‚è¿½åŠ ï¼ˆã€Œãã®ä»–ã€ä»¥å¤–ï¼‰
    for cat in sorted(categories_set):
        if cat not in categories and cat != "ãã®ä»–":
            categories.append(cat)
    
    # ã€Œãã®ä»–ã€ã‚’æœ€å¾Œã«è¿½åŠ 
    if "ãã®ä»–" in categories_set:
        categories.append("ãã®ä»–")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    category_filter = request.args.get('category', '')
    score_filter = request.args.get('score', '')
    search_query = request.args.get('q', '')
    sort_by = request.args.get('sort', 'date')  # ã‚½ãƒ¼ãƒˆé †: date, score, category, source
    page = request.args.get('page', 1, type=int)
    per_page = 30  # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®ä»¶æ•°
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_news = news_list
    
    if category_filter:
        # è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰ã«å¯¾å¿œ
        filtered_news = [n for n in filtered_news 
                        if category_filter in [cat.strip() for cat in n["category"].split(",")]]
    
    if score_filter:
        min_score = int(score_filter)
        filtered_news = [n for n in filtered_news if n["score"] >= min_score]
    
    if search_query:
        query_lower = search_query.lower()
        filtered_news = [n for n in filtered_news if 
                        query_lower in n["title"].lower() or 
                        query_lower in n["summary"].lower() or
                        query_lower in n["tags"].lower()]
    
    # ã‚½ãƒ¼ãƒˆæ©Ÿèƒ½
    if sort_by == 'score':
        filtered_news.sort(key=lambda x: (x["score"], parse_date_to_datetime(x["date"])), reverse=True)
    elif sort_by == 'category':
        filtered_news.sort(key=lambda x: (x["category"], parse_date_to_datetime(x["date"])), reverse=True)
    elif sort_by == 'source':
        filtered_news.sort(key=lambda x: (x["source"], parse_date_to_datetime(x["date"])), reverse=True)
    else:  # date (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
        filtered_news.sort(key=lambda x: (parse_date_to_datetime(x["date"]), x["score"]), reverse=True)
    
    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
    total_filtered = len(filtered_news)
    total_pages = (total_filtered + per_page - 1) // per_page  # åˆ‡ã‚Šä¸Šã’
    page = max(1, min(page, total_pages)) if total_pages > 0 else 1
    
    start_idx = (page - 1) * per_page
    end_idx = start_idx + per_page
    paginated_news = filtered_news[start_idx:end_idx]
    
    response = make_response(render_template('index.html', 
                          news_list=paginated_news, 
                          categories=categories,
                          current_category=category_filter,
                          current_score=score_filter,
                          current_search=search_query,
                          current_sort=sort_by,
                          total_count=len(news_list),
                          filtered_count=total_filtered,
                          current_page=page,
                          total_pages=total_pages,
                          per_page=per_page))
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’é˜²ããƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
    response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Expires'] = '0'
    
    return response


@app.route('/api/news')
def api_news():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§API"""
    try:
        news_list = get_all_news()
        logger.debug(f"API: ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚’è¿”å´: {len(news_list)}ä»¶")
        return jsonify(news_list)
    except Exception as e:
        log_exception(logger, e, "API: ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼")
        return jsonify({"error": "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"}), 500


@app.route('/api/stats')
def api_stats():
    """çµ±è¨ˆæƒ…å ±API"""
    try:
        news_list = get_all_news()
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
        category_counts = {}
        source_counts = {}
        score_counts = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
        
        for news in news_list:
            cat = news["category"]
            src = news["source"]
            score = news["score"]
            
            category_counts[cat] = category_counts.get(cat, 0) + 1
            source_counts[src] = source_counts.get(src, 0) + 1
            if score in score_counts:
                score_counts[score] += 1
        
        logger.debug(f"API: çµ±è¨ˆæƒ…å ±ã‚’è¿”å´: ç·æ•°{len(news_list)}ä»¶")
        return jsonify({
            "total": len(news_list),
            "by_category": category_counts,
            "by_source": source_counts,
            "by_score": score_counts
        })
    except Exception as e:
        log_exception(logger, e, "API: çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼")
        return jsonify({"error": "çµ±è¨ˆæƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"}), 500


@app.route('/privacy')
def privacy():
    """ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼"""
    return render_template('privacy.html')


@app.route('/about')
def about():
    """é‹å–¶è€…æƒ…å ±"""
    return render_template('about.html')


@app.route('/contact')
def contact():
    """ãŠå•ã„åˆã‚ã›"""
    return render_template('contact.html')


@app.route('/article/<article_id>')
def article_detail(article_id):
    """è¨˜äº‹è©³ç´°ãƒšãƒ¼ã‚¸"""
    try:
        news_list = get_all_news()
        
        # è¨˜äº‹IDã§è¨˜äº‹ã‚’æ¤œç´¢
        article = None
        for news in news_list:
            if news.get("id") == article_id:
                article = news
                break
        
        if not article:
            logger.warning(f"è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: article_id={article_id}")
            abort(404)
        
        # é–¢é€£è¨˜äº‹ã‚’å–å¾—ï¼ˆåŒã˜ã‚«ãƒ†ã‚´ãƒªã®è¨˜äº‹ã€æœ€å¤§5ä»¶ï¼‰
        related_articles = []
        article_categories = [cat.strip() for cat in article["category"].split(",")]
        for news in news_list:
            if news.get("id") != article_id:
                news_categories = [cat.strip() for cat in news["category"].split(",")]
                # ã‚«ãƒ†ã‚´ãƒªãŒé‡è¤‡ã—ã¦ã„ã‚‹è¨˜äº‹ã‚’é–¢é€£è¨˜äº‹ã¨ã—ã¦è¿½åŠ 
                if any(cat in news_categories for cat in article_categories):
                    related_articles.append(news)
                    if len(related_articles) >= 5:
                        break
        
        # æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆ
        related_articles.sort(key=lambda x: parse_date_to_datetime(x["date"]), reverse=True)
        
        # ãƒ™ãƒ¼ã‚¹URL
        base_url = request.url_root.rstrip('/')
        article_url = f"{base_url}/article/{article_id}"
        
        # æ—¥ä»˜ã‚’ISO 8601å½¢å¼ã«å¤‰æ›
        iso_date = convert_to_iso8601(article.get("date", ""))
        
        logger.info(f"è¨˜äº‹è©³ç´°ãƒšãƒ¼ã‚¸ã‚’è¡¨ç¤º: article_id={article_id}, title={article['title'][:50]}")
        
        return render_template('article.html',
                              article=article,
                              related_articles=related_articles,
                              base_url=base_url,
                              article_url=article_url,
                              iso_date=iso_date)
    except Exception as e:
        log_exception(logger, e, f"è¨˜äº‹è©³ç´°ãƒšãƒ¼ã‚¸è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: article_id={article_id}")
        abort(500)


@app.route('/api/survey', methods=['POST'])
def api_survey():
    """ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã‚’ä¿å­˜"""
    from datetime import datetime
    
    try:
        data = request.get_json()
        if not data:
            logger.warning("API: ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return jsonify({"error": "No data"}), 400
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        survey_file = os.path.join(os.path.dirname(__file__), 'survey_results.csv')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
        file_exists = os.path.exists(survey_file)
        
        with open(survey_file, 'a', encoding='utf-8') as f:
            if not file_exists:
                f.write('timestamp,age,job,industry,source\n')
                logger.info("ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°è¦ä½œæˆã—ã¾ã—ãŸ")
            
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            age = data.get('age', '')
            job = data.get('job', '')
            industry = data.get('industry', '')
            source = data.get('source', '')
            f.write(f'{timestamp},{age},{job},{industry},{source}\n')
        
        logger.info(f"ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã‚’ä¿å­˜ã—ã¾ã—ãŸ: age={age}, job={job}, industry={industry}")
        return jsonify({"status": "ok"})
    except Exception as e:
        log_exception(logger, e, "API: ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼")
        return jsonify({"error": "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ"}), 500


def get_survey_data():
    """ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§é›†è¨ˆ"""
    import csv
    from collections import Counter
    
    survey_file = os.path.join(os.path.dirname(__file__), 'survey_results.csv')
    
    if not os.path.exists(survey_file):
        return {
            'total': 0,
            'data': [],
            'stats': {
                'age': {},
                'job': {},
                'industry': {},
                'source': {}
            }
        }
    
    data = []
    age_counts = Counter()
    job_counts = Counter()
    industry_counts = Counter()
    source_counts = Counter()
    
    with open(survey_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # å¤ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆindustryã‚«ãƒ©ãƒ ãŒãªã„ï¼‰ã«å¯¾å¿œ
            normalized_row = {
                'timestamp': row.get('timestamp', ''),
                'age': row.get('age', ''),
                'job': row.get('job', ''),
                'industry': row.get('industry', ''),
                'source': row.get('source', '')
            }
            data.append(normalized_row)
            
            if normalized_row['age']:
                age_counts[normalized_row['age']] += 1
            if normalized_row['job']:
                job_counts[normalized_row['job']] += 1
            if normalized_row['industry']:
                industry_counts[normalized_row['industry']] += 1
            if normalized_row['source']:
                source_counts[normalized_row['source']] += 1
    
    return {
        'total': len(data),
        'data': data,
        'stats': {
            'age': dict(age_counts),
            'job': dict(job_counts),
            'industry': dict(industry_counts),
            'source': dict(source_counts)
        }
    }


@app.route('/admin')
def admin():
    """ç®¡ç†ç”»é¢"""
    survey_data = get_survey_data()
    return render_template('admin.html', survey=survey_data)


@app.route('/api/clear-cache', methods=['POST'])
def clear_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
    global _news_cache, _cache_timestamp
    _news_cache = None
    _cache_timestamp = None
    return jsonify({"status": "ok", "message": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ"})


@app.route('/api/refresh-news', methods=['POST'])
def refresh_news():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å¼·åˆ¶çš„ã«å†å–å¾—"""
    news_list = get_all_news(use_cache=False)  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å–å¾—
    return jsonify({
        "status": "ok", 
        "message": "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¾ã—ãŸ",
        "count": len(news_list)
    })


@app.route('/sitemap.xml')
def sitemap():
    """XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
    try:
        import xml.etree.ElementTree as ET
        from datetime import datetime
        
        # ãƒ«ãƒ¼ãƒˆè¦ç´ ã‚’ä½œæˆ
        urlset = ET.Element('urlset')
        urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
        
        # ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã‚’å«ã‚ã‚‹
        base_url = 'https://biz-ai-news.onrender.com'
        
        url = ET.SubElement(urlset, 'url')
        ET.SubElement(url, 'loc').text = base_url
        ET.SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
        ET.SubElement(url, 'changefreq').text = 'hourly'
        ET.SubElement(url, 'priority').text = '1.0'
        
        # è¨˜äº‹ã®å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚’ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã«è¿½åŠ ï¼ˆæœ€æ–°100ä»¶ï¼‰
        try:
            news_list = get_all_news()
            for news in news_list[:100]:  # æœ€æ–°100ä»¶ã®ã¿
                if news.get('id'):
                    article_url = f"{base_url}/article/{news['id']}"
                    url = ET.SubElement(urlset, 'url')
                    ET.SubElement(url, 'loc').text = article_url
                    # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦lastmodã«è¨­å®š
                    try:
                        # æ—¥ä»˜å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆè¤‡æ•°å½¢å¼ã«å¯¾å¿œï¼‰
                        date_str = news.get('date', '')
                        if date_str:
                            # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                            parsed_date = date_str.split()[0] if ' ' in date_str else date_str
                            ET.SubElement(url, 'lastmod').text = parsed_date
                        else:
                            ET.SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
                    except:
                        ET.SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
                    ET.SubElement(url, 'changefreq').text = 'weekly'
                    ET.SubElement(url, 'priority').text = '0.8'
        except Exception as e:
            log_exception(logger, e, "ã‚µã‚¤ãƒˆãƒãƒƒãƒ—: è¨˜äº‹ãƒšãƒ¼ã‚¸ã®è¿½åŠ ã‚¨ãƒ©ãƒ¼")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã®URLã¯å«ã‚ã‚‹
        
        # XMLã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        xml_str = ET.tostring(urlset, encoding='utf-8', method='xml')
        xml_str = b'<?xml version="1.0" encoding="UTF-8"?>\n' + xml_str
        
        return Response(xml_str, mimetype='application/xml')
    except Exception as e:
        log_exception(logger, e, "ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆã‚¨ãƒ©ãƒ¼")
        return Response('<?xml version="1.0" encoding="UTF-8"?><urlset></urlset>', mimetype='application/xml')


@app.route('/robots.txt')
def robots():
    """robots.txtã‚’è¿”ã™"""
    try:
        robots_content = """User-agent: *
Allow: /
Disallow: /admin
Disallow: /api/

Sitemap: https://biz-ai-news.onrender.com/sitemap.xml
"""
        return Response(robots_content, mimetype='text/plain')
    except Exception as e:
        log_exception(logger, e, "robots.txtç”Ÿæˆã‚¨ãƒ©ãƒ¼")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚åŸºæœ¬çš„ãªrobots.txtã‚’è¿”ã™
        robots_content = """User-agent: *
Allow: /

Sitemap: https://biz-ai-news.onrender.com/sitemap.xml
"""
        return Response(robots_content, mimetype='text/plain')


@app.route('/ads.txt')
def ads_txt():
    """ads.txtã‚’è¿”ã™ï¼ˆGoogle AdSenseç”¨ï¼‰"""
    try:
        ads_content = "google.com, pub-1202448154240053, DIRECT, f08c47fec0942fa0\n"
        return Response(ads_content, mimetype='text/plain')
    except Exception as e:
        log_exception(logger, e, "ads.txtç”Ÿæˆã‚¨ãƒ©ãƒ¼")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚åŸºæœ¬çš„ãªads.txtã‚’è¿”ã™
        ads_content = "google.com, pub-1202448154240053, DIRECT, f08c47fec0942fa0\n"
        return Response(ads_content, mimetype='text/plain')


@app.errorhandler(404)
def not_found(error):
    """404ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    return render_template('404.html'), 404


@app.errorhandler(500)
def internal_error(error):
    """500ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    log_exception(logger, error, "500ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    return render_template('500.html'), 500


if __name__ == '__main__':
    # æœ¬ç•ªç’°å¢ƒç”¨ã®è¨­å®š
    port = int(os.environ.get('PORT', 5001))
    debug = os.environ.get('FLASK_DEBUG', 'False').lower() == 'true'
    
    print("ğŸš€ AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ã‚’èµ·å‹•ä¸­...")
    if debug:
        print(f"   http://localhost:{port} ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„")
    else:
        print(f"   æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ä¸­ï¼ˆãƒãƒ¼ãƒˆ: {port}ï¼‰")
    
    logger.info(f"Webã‚¢ãƒ—ãƒªã‚’èµ·å‹•: ãƒãƒ¼ãƒˆ={port}, ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰={debug}")
    
    app.run(debug=debug, host='0.0.0.0', port=port)

