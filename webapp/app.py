#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ« - Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""

import os
import sys
import traceback
from flask import Flask, render_template, jsonify, request
from datetime import datetime

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import gspread
from google.oauth2.credentials import Credentials

# ãƒ­ã‚°è¨­å®š
from logger_config import get_webapp_logger, log_exception

app = Flask(__name__)

# ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–
logger = get_webapp_logger()

# è¨­å®š
SPREADSHEET_NAME = "AIãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ï¼ˆãƒãƒ«ãƒã‚µã‚¤ãƒˆï¼‰"

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—ï¼ˆRenderç”¨ï¼‰
# ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€
GOOGLE_SHEETS_TOKEN = os.getenv('GOOGLE_SHEETS_TOKEN')
GOOGLE_SHEETS_CREDENTIALS = os.getenv('GOOGLE_SHEETS_CREDENTIALS')

# ãƒˆãƒ¼ã‚¯ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨ï¼‰
TOKEN_FILE = os.path.join(os.path.dirname(os.path.dirname(__file__)), "token.json")
CREDENTIALS_FILE = os.path.join(os.path.dirname(os.path.dirname(__file__)), "credentials.json")

SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive.file']


def get_sheets_client():
    """Google Sheets ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—"""
    try:
        import json
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—ï¼ˆRenderç”¨ï¼‰
        if GOOGLE_SHEETS_TOKEN:
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èª­ã¿è¾¼ã‚€
            token_data = json.loads(GOOGLE_SHEETS_TOKEN)
            creds = Credentials.from_authorized_user_info(token_data, SCOPES)
            logger.info("ç’°å¢ƒå¤‰æ•°ã‹ã‚‰Googleèªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        elif os.path.exists(TOKEN_FILE):
            # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ: ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€
            creds = Credentials.from_authorized_user_file(TOKEN_FILE, SCOPES)
            logger.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Googleèªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        else:
            raise FileNotFoundError("Googleèªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°GOOGLE_SHEETS_TOKENã¾ãŸã¯token.jsonãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚")
        
        return gspread.authorize(creds)
    except FileNotFoundError as e:
        log_exception(logger, e, "Google Sheetsèªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        raise
    except json.JSONDecodeError as e:
        log_exception(logger, e, "Googleèªè¨¼æƒ…å ±ã®JSONè§£æã‚¨ãƒ©ãƒ¼")
        raise ValueError("GOOGLE_SHEETS_TOKENã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“") from e
    except Exception as e:
        log_exception(logger, e, "Google Sheetsèªè¨¼ã‚¨ãƒ©ãƒ¼")
        raise


# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
_news_cache = None
_cache_timestamp = None
CACHE_DURATION = 60  # 1åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆçŸ­ç¸®ã—ã¦æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’åæ˜ ã—ã‚„ã™ãï¼‰

def get_all_news(use_cache=True):
    """å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰"""
    global _news_cache, _cache_timestamp
    
    import time
    current_time = time.time()
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ãªå ´åˆã¯è¿”ã™
    if use_cache and _news_cache is not None and _cache_timestamp is not None:
        if current_time - _cache_timestamp < CACHE_DURATION:
            return _news_cache
    
    try:
        client = get_sheets_client()
        spreadsheet = client.open(SPREADSHEET_NAME)
        logger.debug("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ãã¾ã—ãŸ")
    except Exception as e:
        log_exception(logger, e, "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ©ãƒ¼")
        raise
    
    all_news = []
    
    # ãƒãƒƒãƒã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ï¼‰
    try:
        for worksheet in spreadsheet.worksheets():
            category = worksheet.title
            try:
                # get_all_values()ã¯ä¸€åº¦ã«å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆåŠ¹ç‡çš„ï¼‰
                rows = worksheet.get_all_values()
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
                for row in rows[1:]:
                    if len(row) >= 7:
                        # ã‚¹ã‚³ã‚¢ã‚’æ•°å€¤ã«å¤‰æ›
                        score_str = row[5] if len(row) > 5 else ""
                        score = score_str.count("â­")
                        
                        # URLã‚’å–å¾—ï¼ˆIåˆ—ã«å®ŸURLãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°Googleæ¤œç´¢ãƒªãƒ³ã‚¯ï¼‰
                        url = ""
                        if len(row) > 8 and row[8].startswith("http"):
                            url = row[8]
                        elif len(row) > 7 and row[7].startswith("http"):
                            url = row[7]
                        else:
                            # Googleæ¤œç´¢ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ
                            import urllib.parse
                            title = row[2]
                            url = f"https://www.google.com/search?q={urllib.parse.quote(title)}"
                        
                        # ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—ï¼ˆJåˆ—ã«è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ã‚·ãƒ¼ãƒˆåï¼‰
                        article_category = category
                        if len(row) > 9 and row[9]:
                            article_category = row[9]  # Jåˆ—ã®ã‚«ãƒ†ã‚´ãƒªæƒ…å ±
                        
                        news_item = {
                            "no": row[0],
                            "source": row[1],
                            "title": row[2],
                            "date": row[3],
                            "tags": row[4],
                            "score": score,
                            "score_display": score_str,
                            "summary": row[6],
                            "url": url,
                            "category": article_category  # è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªã®å ´åˆã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š
                        }
                        all_news.append(news_item)
            except Exception as e:
                log_exception(logger, e, f"ã‚·ãƒ¼ãƒˆã€Œ{category}ã€ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼")
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ä»–ã®ã‚·ãƒ¼ãƒˆã®å‡¦ç†ã¯ç¶šè¡Œ
                continue
        
        # æ—¥ä»˜é †ï¼ˆæ–°ã—ã„é †ï¼‰ã€åŒæ—¥å†…ã¯ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        all_news.sort(key=lambda x: (x["date"], x["score"]), reverse=True)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        _news_cache = all_news
        _cache_timestamp = current_time
        
        logger.info(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ: {len(all_news)}ä»¶")
        return all_news
    except Exception as e:
        log_exception(logger, e, "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼")
        raise


@app.route('/')
def index():
    """ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸"""
    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚Œã°å¼·åˆ¶æ›´æ–°
        force_refresh = request.args.get('refresh', '').lower() == 'true'
        if force_refresh:
            logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å¼·åˆ¶æ›´æ–°")
        news_list = get_all_news(use_cache=not force_refresh)
    except Exception as e:
        log_exception(logger, e, "ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸è¡¨ç¤ºã‚¨ãƒ©ãƒ¼")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        news_list = []
        logger.error("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™ã€‚")
    
    # ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã‚«ãƒ†ã‚´ãƒªã®å ´åˆã¯åˆ†å‰²ï¼‰
    categories_set = set()
    for n in news_list:
        for cat in [c.strip() for c in n["category"].split(",")]:
            categories_set.add(cat)
    
    # ã‚«ãƒ†ã‚´ãƒªã®è¡¨ç¤ºé †åºã‚’å›ºå®šï¼ˆå„ªå…ˆé †ä½é †ã€ã€Œãã®ä»–ã€ã‚’æœ€å¾Œã«ï¼‰
    # ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ãªãã¦ã‚‚ã€å›ºå®šã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆã‚’è¡¨ç¤º
    category_order = [
        "ä¼æ¥­åŠ¹ç‡åŒ–",
        "DXãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–",
        "ä¼æ¥­å°å…¥",
        "AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼"
    ]
    
    # å›ºå®šã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆã‚’å¸¸ã«è¡¨ç¤ºï¼ˆãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ãªã„å ´åˆã§ã‚‚ï¼‰
    categories = category_order.copy()
    # å›ºå®šé †åºã«ãªã„ã‚«ãƒ†ã‚´ãƒªã‚‚è¿½åŠ ï¼ˆã€Œãã®ä»–ã€ä»¥å¤–ï¼‰
    for cat in sorted(categories_set):
        if cat not in categories and cat != "ãã®ä»–":
            categories.append(cat)
    
    # ã€Œãã®ä»–ã€ã‚’æœ€å¾Œã«è¿½åŠ 
    if "ãã®ä»–" in categories_set:
        categories.append("ãã®ä»–")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    category_filter = request.args.get('category', '')
    score_filter = request.args.get('score', '')
    search_query = request.args.get('q', '')
    sort_by = request.args.get('sort', 'date')  # ã‚½ãƒ¼ãƒˆé †: date, score, category, source
    page = request.args.get('page', 1, type=int)
    per_page = 30  # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®ä»¶æ•°
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_news = news_list
    
    if category_filter:
        # è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰ã«å¯¾å¿œ
        filtered_news = [n for n in filtered_news 
                        if category_filter in [cat.strip() for cat in n["category"].split(",")]]
    
    if score_filter:
        min_score = int(score_filter)
        filtered_news = [n for n in filtered_news if n["score"] >= min_score]
    
    if search_query:
        query_lower = search_query.lower()
        filtered_news = [n for n in filtered_news if 
                        query_lower in n["title"].lower() or 
                        query_lower in n["summary"].lower() or
                        query_lower in n["tags"].lower()]
    
    # ã‚½ãƒ¼ãƒˆæ©Ÿèƒ½
    if sort_by == 'score':
        filtered_news.sort(key=lambda x: (x["score"], x["date"]), reverse=True)
    elif sort_by == 'category':
        filtered_news.sort(key=lambda x: (x["category"], x["date"]), reverse=True)
    elif sort_by == 'source':
        filtered_news.sort(key=lambda x: (x["source"], x["date"]), reverse=True)
    else:  # date (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
        filtered_news.sort(key=lambda x: (x["date"], x["score"]), reverse=True)
    
    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
    total_filtered = len(filtered_news)
    total_pages = (total_filtered + per_page - 1) // per_page  # åˆ‡ã‚Šä¸Šã’
    page = max(1, min(page, total_pages)) if total_pages > 0 else 1
    
    start_idx = (page - 1) * per_page
    end_idx = start_idx + per_page
    paginated_news = filtered_news[start_idx:end_idx]
    
    return render_template('index.html', 
                          news_list=paginated_news, 
                          categories=categories,
                          current_category=category_filter,
                          current_score=score_filter,
                          current_search=search_query,
                          current_sort=sort_by,
                          total_count=len(news_list),
                          filtered_count=total_filtered,
                          current_page=page,
                          total_pages=total_pages,
                          per_page=per_page)


@app.route('/api/news')
def api_news():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§API"""
    try:
        news_list = get_all_news()
        logger.debug(f"API: ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚’è¿”å´: {len(news_list)}ä»¶")
        return jsonify(news_list)
    except Exception as e:
        log_exception(logger, e, "API: ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼")
        return jsonify({"error": "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"}), 500


@app.route('/api/stats')
def api_stats():
    """çµ±è¨ˆæƒ…å ±API"""
    try:
        news_list = get_all_news()
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
        category_counts = {}
        source_counts = {}
        score_counts = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
        
        for news in news_list:
            cat = news["category"]
            src = news["source"]
            score = news["score"]
            
            category_counts[cat] = category_counts.get(cat, 0) + 1
            source_counts[src] = source_counts.get(src, 0) + 1
            if score in score_counts:
                score_counts[score] += 1
        
        logger.debug(f"API: çµ±è¨ˆæƒ…å ±ã‚’è¿”å´: ç·æ•°{len(news_list)}ä»¶")
        return jsonify({
            "total": len(news_list),
            "by_category": category_counts,
            "by_source": source_counts,
            "by_score": score_counts
        })
    except Exception as e:
        log_exception(logger, e, "API: çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼")
        return jsonify({"error": "çµ±è¨ˆæƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"}), 500


@app.route('/privacy')
def privacy():
    """ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼"""
    return render_template('privacy.html')


@app.route('/about')
def about():
    """é‹å–¶è€…æƒ…å ±"""
    return render_template('about.html')


@app.route('/contact')
def contact():
    """ãŠå•ã„åˆã‚ã›"""
    return render_template('contact.html')


@app.route('/api/survey', methods=['POST'])
def api_survey():
    """ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã‚’ä¿å­˜"""
    from datetime import datetime
    
    try:
        data = request.get_json()
        if not data:
            logger.warning("API: ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return jsonify({"error": "No data"}), 400
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        survey_file = os.path.join(os.path.dirname(__file__), 'survey_results.csv')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
        file_exists = os.path.exists(survey_file)
        
        with open(survey_file, 'a', encoding='utf-8') as f:
            if not file_exists:
                f.write('timestamp,age,job,industry,source\n')
                logger.info("ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°è¦ä½œæˆã—ã¾ã—ãŸ")
            
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            age = data.get('age', '')
            job = data.get('job', '')
            industry = data.get('industry', '')
            source = data.get('source', '')
            f.write(f'{timestamp},{age},{job},{industry},{source}\n')
        
        logger.info(f"ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã‚’ä¿å­˜ã—ã¾ã—ãŸ: age={age}, job={job}, industry={industry}")
        return jsonify({"status": "ok"})
    except Exception as e:
        log_exception(logger, e, "API: ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼")
        return jsonify({"error": "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ"}), 500


def get_survey_data():
    """ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§é›†è¨ˆ"""
    import csv
    from collections import Counter
    
    survey_file = os.path.join(os.path.dirname(__file__), 'survey_results.csv')
    
    if not os.path.exists(survey_file):
        return {
            'total': 0,
            'data': [],
            'stats': {
                'age': {},
                'job': {},
                'industry': {},
                'source': {}
            }
        }
    
    data = []
    age_counts = Counter()
    job_counts = Counter()
    industry_counts = Counter()
    source_counts = Counter()
    
    with open(survey_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # å¤ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆindustryã‚«ãƒ©ãƒ ãŒãªã„ï¼‰ã«å¯¾å¿œ
            normalized_row = {
                'timestamp': row.get('timestamp', ''),
                'age': row.get('age', ''),
                'job': row.get('job', ''),
                'industry': row.get('industry', ''),
                'source': row.get('source', '')
            }
            data.append(normalized_row)
            
            if normalized_row['age']:
                age_counts[normalized_row['age']] += 1
            if normalized_row['job']:
                job_counts[normalized_row['job']] += 1
            if normalized_row['industry']:
                industry_counts[normalized_row['industry']] += 1
            if normalized_row['source']:
                source_counts[normalized_row['source']] += 1
    
    return {
        'total': len(data),
        'data': data,
        'stats': {
            'age': dict(age_counts),
            'job': dict(job_counts),
            'industry': dict(industry_counts),
            'source': dict(source_counts)
        }
    }


@app.route('/admin')
def admin():
    """ç®¡ç†ç”»é¢"""
    survey_data = get_survey_data()
    return render_template('admin.html', survey=survey_data)


@app.route('/api/clear-cache', methods=['POST'])
def clear_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
    global _news_cache, _cache_timestamp
    _news_cache = None
    _cache_timestamp = None
    return jsonify({"status": "ok", "message": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ"})


@app.route('/api/refresh-news', methods=['POST'])
def refresh_news():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å¼·åˆ¶çš„ã«å†å–å¾—"""
    news_list = get_all_news(use_cache=False)  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å–å¾—
    return jsonify({
        "status": "ok", 
        "message": "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¾ã—ãŸ",
        "count": len(news_list)
    })


@app.route('/sitemap.xml')
def sitemap():
    """XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
    try:
        from flask import Response
        import xml.etree.ElementTree as ET
        from datetime import datetime
        
        # ãƒ«ãƒ¼ãƒˆè¦ç´ ã‚’ä½œæˆ
        urlset = ET.Element('urlset')
        urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
        
        # ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸
        url = ET.SubElement(urlset, 'url')
        ET.SubElement(url, 'loc').text = 'https://biz-ai-news.onrender.com'
        ET.SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
        ET.SubElement(url, 'changefreq').text = 'hourly'
        ET.SubElement(url, 'priority').text = '1.0'
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ï¼ˆæœ€æ–°50ä»¶ï¼‰
        news_list = get_all_news()
        for news in news_list[:50]:  # æœ€æ–°50ä»¶ã®ã¿
            url = ET.SubElement(urlset, 'url')
            # è¨˜äº‹ã®URLï¼ˆå®Ÿéš›ã®URLã‚’ä½¿ç”¨ï¼‰
            article_url = news.get('url', 'https://biz-ai-news.onrender.com')
            ET.SubElement(url, 'loc').text = article_url
            # æ—¥ä»˜
            if news.get('date'):
                try:
                    # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ISOå½¢å¼ã«å¤‰æ›
                    date_str = news['date']
                    if isinstance(date_str, str):
                        # æ§˜ã€…ãªæ—¥ä»˜å½¢å¼ã«å¯¾å¿œ
                        date_obj = datetime.strptime(date_str.split()[0], '%Y-%m-%d')
                        ET.SubElement(url, 'lastmod').text = date_obj.strftime('%Y-%m-%d')
                    else:
                        ET.SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
                except:
                    ET.SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
            else:
                ET.SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
            ET.SubElement(url, 'changefreq').text = 'weekly'
            ET.SubElement(url, 'priority').text = '0.8'
        
        # XMLã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        xml_str = ET.tostring(urlset, encoding='utf-8', method='xml')
        xml_str = b'<?xml version="1.0" encoding="UTF-8"?>\n' + xml_str
        
        return Response(xml_str, mimetype='application/xml')
    except Exception as e:
        log_exception(logger, e, "ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆã‚¨ãƒ©ãƒ¼")
        return Response('<?xml version="1.0" encoding="UTF-8"?><urlset></urlset>', mimetype='application/xml')


@app.route('/robots.txt')
def robots():
    """robots.txtã‚’è¿”ã™"""
    robots_content = """User-agent: *
Allow: /
Disallow: /admin
Disallow: /api/

Sitemap: https://biz-ai-news.onrender.com/sitemap.xml
"""
    return Response(robots_content, mimetype='text/plain')


if __name__ == '__main__':
    # æœ¬ç•ªç’°å¢ƒç”¨ã®è¨­å®š
    port = int(os.environ.get('PORT', 5001))
    debug = os.environ.get('FLASK_DEBUG', 'False').lower() == 'true'
    
    print("ğŸš€ AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ã‚’èµ·å‹•ä¸­...")
    if debug:
        print(f"   http://localhost:{port} ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„")
    else:
        print(f"   æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ä¸­ï¼ˆãƒãƒ¼ãƒˆ: {port}ï¼‰")
    
    logger.info(f"Webã‚¢ãƒ—ãƒªã‚’èµ·å‹•: ãƒãƒ¼ãƒˆ={port}, ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰={debug}")
    
    app.run(debug=debug, host='0.0.0.0', port=port)

