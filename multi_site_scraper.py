#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒãƒ«ãƒã‚µã‚¤ãƒˆå¯¾å¿œ AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° & è¦ç´„ãƒ„ãƒ¼ãƒ«

å¯¾å¿œã‚µã‚¤ãƒˆ:
- Ledge.ai
- AINOW
- PR TIMES
- ITmedia AI+
"""

import os
import re
import time
import traceback
from datetime import datetime, timedelta
from typing import Optional
from dotenv import load_dotenv
from openai import OpenAI

# Google Sheetsé–¢é€£
import gspread
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
from scrapers import LedgeAiScraper, AINowScraper, PRTimesScraper, ITmediaAiPlusScraper

# è¨­å®š: 1å›ã®å®Ÿè¡Œã§è¿½åŠ ã™ã‚‹è¨˜äº‹æ•°ã®ä¸Šé™
# ç’°å¢ƒå¤‰æ•° MAX_ARTICLES_PER_RUN ã§å¤‰æ›´å¯èƒ½ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ä»¶ï¼‰
MAX_ARTICLES_PER_RUN = int(os.getenv('MAX_ARTICLES_PER_RUN', '10'))

# è¨­å®š: å¤ã„è¨˜äº‹ã‚’è‡ªå‹•å‰Šé™¤ã™ã‚‹æœŸé–“ï¼ˆæ—¥æ•°ï¼‰
# ç’°å¢ƒå¤‰æ•° ARTICLE_RETENTION_DAYS ã§å¤‰æ›´å¯èƒ½ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 45æ—¥ï¼‰
ARTICLE_RETENTION_DAYS = int(os.getenv('ARTICLE_RETENTION_DAYS', '45'))

# ãƒ­ã‚°è¨­å®š
from logger_config import get_scraper_logger, log_exception

# ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–
logger = get_scraper_logger()

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# Google Sheets APIã®ã‚¹ã‚³ãƒ¼ãƒ—
SCOPES = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive.file'
]

# èªè¨¼æƒ…å ±ã®ãƒ‘ã‚¹
CREDENTIALS_FILE = "/Users/masak/Desktop/ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°/credentials.json"
TOKEN_FILE = "/Users/masak/Desktop/ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°/token.json"

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå
SPREADSHEET_NAME = "AIãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ï¼ˆãƒãƒ«ãƒã‚µã‚¤ãƒˆï¼‰"

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®šï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰
KEYWORD_CATEGORIES = {
    "ä¼æ¥­åŠ¹ç‡åŒ–": [
        "æ¥­å‹™åŠ¹ç‡åŒ–", "æ¥­å‹™æ”¹å–„", "ç”Ÿç”£æ€§å‘ä¸Š", "ã‚³ã‚¹ãƒˆå‰Šæ¸›", "åƒãæ–¹æ”¹é©",
        "è‡ªå‹•åŒ–", "åŠ¹ç‡åŒ–", "çœåŠ›åŒ–", "æ™‚çŸ­",
    ],
    "DXãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–": [
        "DX", "ãƒ‡ã‚¸ã‚¿ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³", "ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–", "ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©",
    ],
    "ä¼æ¥­å°å…¥": [
        "ä¼æ¥­å°å…¥", "ä¼æ¥­äº‹ä¾‹", "å›½å†…ä¼æ¥­", "å°å…¥äº‹ä¾‹", "æ´»ç”¨äº‹ä¾‹", "ãƒ“ã‚¸ãƒã‚¹æ´»ç”¨",
    ],
    "AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼": [
        "AI", "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ç”ŸæˆAI", "ChatGPT", "GPT", "LLM",
        "AIå°å…¥", "AIæ´»ç”¨", "ãƒ‡ãƒ¼ã‚¿åˆ†æ",
    ],
}

# å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ï¼‰
KEYWORDS = [
    keyword for keywords in KEYWORD_CATEGORIES.values() for keyword in keywords
]


class ArticleSummarizer:
    """OpenAI APIã‚’ä½¿ç”¨ã—ã¦è¨˜äº‹ã‚’è¦ç´„ãƒ»è©•ä¾¡ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)

    def summarize_and_score(self, article: dict) -> dict:
        """è¨˜äº‹ã‚’è¦ç´„ã—ã€é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’ä»˜ã‘ã‚‹"""
        content = article.get("content", "")
        title = article.get("title", "")
        
        if not content:
            return {"summary": "è¨˜äº‹æœ¬æ–‡ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", "score": 1}
        
        prompt = f"""ä»¥ä¸‹ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘
{title}

ã€æœ¬æ–‡ã€‘
{content[:3000]}

---
ä»¥ä¸‹ã®2ã¤ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

## 1. è¦ç´„ï¼ˆ150ã€œ200æ–‡å­—ï¼‰
- ä½•ãŒç™ºè¡¨/ç™ºç”Ÿã—ãŸã®ã‹ï¼ˆWho/Whatï¼‰
- ãƒ“ã‚¸ãƒã‚¹ã¸ã®å½±éŸ¿ã‚„æ„ç¾©
- ä»Šå¾Œã®å±•æœ›ï¼ˆã‚ã‚Œã°ï¼‰

## 2. é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆ1ã€œ5ã®æ•´æ•°ï¼‰
ä»¥ä¸‹ã®åŸºæº–ã§è©•ä¾¡ï¼š
- 5: æ¥­ç•Œå…¨ä½“ã«å½±éŸ¿ã™ã‚‹é‡å¤§ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆå¤§æ‰‹ä¼æ¥­ã®å¤§è¦æ¨¡å°å…¥ã€ç”»æœŸçš„ãªæŠ€è¡“ç™ºè¡¨ãªã©ï¼‰
- 4: æ³¨ç›®ã™ã¹ãé‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆå…·ä½“çš„ãªæˆæœãƒ»æ•°å€¤ã‚ã‚Šã€å›½å†…å¤§æ‰‹ä¼æ¥­ã®äº‹ä¾‹ï¼‰
- 3: å‚è€ƒã«ãªã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆä¸€èˆ¬çš„ãªå°å…¥äº‹ä¾‹ã€æŠ€è¡“è§£èª¬ï¼‰
- 2: è»½ã„æƒ…å ±ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆå‘ŠçŸ¥ã€å°è¦æ¨¡ãªå–ã‚Šçµ„ã¿ï¼‰
- 1: é‡è¦åº¦ä½ã„ï¼ˆãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ã®ã¿ã€å†…å®¹è–„ã„ï¼‰

---
ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
ã€è¦ç´„ã€‘
ï¼ˆè¦ç´„æ–‡ï¼‰

ã€ã‚¹ã‚³ã‚¢ã€‘
ï¼ˆ1ã€œ5ã®æ•°å­—ã®ã¿ï¼‰"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼åˆ†é‡ã«ç²¾é€šã—ãŸãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’çš„ç¢ºã«è¦ç´„ã—ã€ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã«ã¨ã£ã¦ã®é‡è¦åº¦ã‚’è©•ä¾¡ã—ã¾ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=600,
                temperature=0.3
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # è¦ç´„ã¨ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
            summary = ""
            score = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
            if "ã€è¦ç´„ã€‘" in result_text:
                parts = result_text.split("ã€ã‚¹ã‚³ã‚¢ã€‘")
                summary_part = parts[0].replace("ã€è¦ç´„ã€‘", "").strip()
                summary = summary_part
                
                if len(parts) > 1:
                    score_text = parts[1].strip()
                    # æ•°å­—ã‚’æŠ½å‡º
                    for char in score_text:
                        if char.isdigit():
                            score = int(char)
                            break
            else:
                summary = result_text
            
            # ã‚¹ã‚³ã‚¢ã‚’1-5ã®ç¯„å›²ã«åˆ¶é™
            score = max(1, min(5, score))
            
            return {"summary": summary, "score": score}
            
        except Exception as e:
            log_exception(logger, e, f"OpenAI APIè¦ç´„ã‚¨ãƒ©ãƒ¼ (ã‚¿ã‚¤ãƒˆãƒ«: {title[:50]})")
            return {"summary": f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)}", "score": 1}


class GoogleSheetsExporter:
    """çµæœã‚’Google Sheetsã«å‡ºåŠ›ã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚·ãƒ¼ãƒˆå¯¾å¿œï¼‰"""
    
    def __init__(self):
        self.creds = None
        self.client = None
        self.spreadsheet = None
        self.worksheets = {}  # ã‚«ãƒ†ã‚´ãƒªå -> worksheet
        self.existing_urls = set()
        self.existing_titles = set()
        self.normalized_urls = {}  # æ­£è¦åŒ–ã•ã‚ŒãŸURL -> å…ƒã®URL
        self.normalized_titles = {}  # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ« -> å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«
        
        self._authenticate()
        self._setup_spreadsheet()
    
    def _authenticate(self):
        """Googleèªè¨¼ã‚’è¡Œã†"""
        print("ğŸ” Googleèªè¨¼ä¸­...")
        logger.info("Googleèªè¨¼ã‚’é–‹å§‹")
        
        try:
            if os.path.exists(TOKEN_FILE):
                self.creds = Credentials.from_authorized_user_file(TOKEN_FILE, SCOPES)
            
            if not self.creds or not self.creds.valid:
                if self.creds and self.creds.expired and self.creds.refresh_token:
                    logger.info("ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ä¸­")
                    self.creds.refresh(Request())
                else:
                    logger.info("æ–°ã—ã„èªè¨¼ãƒ•ãƒ­ãƒ¼ã‚’é–‹å§‹")
                    flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_FILE, SCOPES)
                    self.creds = flow.run_local_server(port=0)
                
                with open(TOKEN_FILE, 'w') as token:
                    token.write(self.creds.to_json())
                logger.info("ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            
            self.client = gspread.authorize(self.creds)
            print("âœ… Googleèªè¨¼å®Œäº†")
            logger.info("Googleèªè¨¼å®Œäº†")
        except FileNotFoundError as e:
            error_msg = f"èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {CREDENTIALS_FILE}"
            log_exception(logger, e, "Googleèªè¨¼ã‚¨ãƒ©ãƒ¼")
            raise FileNotFoundError(error_msg) from e
        except Exception as e:
            log_exception(logger, e, "Googleèªè¨¼ã‚¨ãƒ©ãƒ¼")
            raise
    
    def _setup_spreadsheet(self):
        """ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚·ãƒ¼ãƒˆï¼‰"""
        is_new = False
        try:
            self.spreadsheet = self.client.open(SPREADSHEET_NAME)
            print(f"ğŸ“‚ æ—¢å­˜ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ãã¾ã—ãŸ: {SPREADSHEET_NAME}")
            logger.info(f"æ—¢å­˜ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ãã¾ã—ãŸ: {SPREADSHEET_NAME}")
        except gspread.SpreadsheetNotFound:
            print(f"ğŸ“„ æ–°è¦ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ä½œæˆ: {SPREADSHEET_NAME}")
            logger.info(f"æ–°è¦ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ä½œæˆ: {SPREADSHEET_NAME}")
            try:
                self.spreadsheet = self.client.create(SPREADSHEET_NAME)
                is_new = True
                print(f"ğŸ”— ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURL: {self.spreadsheet.url}")
                logger.info(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURL: {self.spreadsheet.url}")
            except Exception as e:
                log_exception(logger, e, "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼")
                raise
        except Exception as e:
            log_exception(logger, e, "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ©ãƒ¼")
            raise
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚·ãƒ¼ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        category_names = list(KEYWORD_CATEGORIES.keys()) + ["ãã®ä»–"]
        
        for category in category_names:
            try:
                ws = self.spreadsheet.worksheet(category)
                self.worksheets[category] = ws
                logger.debug(f"ã‚·ãƒ¼ãƒˆã€Œ{category}ã€ã‚’èª­ã¿è¾¼ã¿")
            except gspread.WorksheetNotFound:
                try:
                    ws = self.spreadsheet.add_worksheet(title=category, rows=200, cols=10)
                    self.worksheets[category] = ws
                    self._setup_sheet_headers(ws, category)
                    logger.info(f"æ–°è¦ã‚·ãƒ¼ãƒˆã€Œ{category}ã€ã‚’ä½œæˆã—ã¾ã—ãŸ")
                except Exception as e:
                    log_exception(logger, e, f"ã‚·ãƒ¼ãƒˆã€Œ{category}ã€ä½œæˆã‚¨ãƒ©ãƒ¼")
                    raise
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ãƒ¼ãƒˆ1ã‚’å‰Šé™¤ï¼ˆæ–°è¦ã®å ´åˆï¼‰
        if is_new:
            try:
                default_sheet = self.spreadsheet.worksheet("ã‚·ãƒ¼ãƒˆ1")
                self.spreadsheet.del_worksheet(default_sheet)
                logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒ¼ãƒˆã€Œã‚·ãƒ¼ãƒˆ1ã€ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            except Exception as e:
                logger.warning(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒ¼ãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç„¡è¦–ï¼‰: {e}")
        
        # æ—¢å­˜URLã‚’èª­ã¿è¾¼ã¿
        self._load_existing_urls()
    
    def _setup_sheet_headers(self, worksheet, category: str):
        """ã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€ãƒ¼ã¨æ›¸å¼ã‚’è¨­å®š"""
        headers = ["No.", "ã‚½ãƒ¼ã‚¹", "ã‚¿ã‚¤ãƒˆãƒ«", "æ—¥ä»˜", "ã‚¿ã‚°", "é‡è¦åº¦", "è¦ç´„", "URL", "å®ŸURL", "ã‚«ãƒ†ã‚´ãƒª"]
        worksheet.update(values=[headers], range_name='A1:J1')
        
        # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®è‰²è¨­å®š
        colors = {
            "ä¼æ¥­åŠ¹ç‡åŒ–": {'red': 0.15, 'green': 0.35, 'blue': 0.55},
            "DXãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–": {'red': 0.25, 'green': 0.45, 'blue': 0.30},
            "ä¼æ¥­å°å…¥": {'red': 0.50, 'green': 0.30, 'blue': 0.45},
            "AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼": {'red': 0.30, 'green': 0.20, 'blue': 0.60},
            "ãã®ä»–": {'red': 0.40, 'green': 0.40, 'blue': 0.40},
        }
        bg_color = colors.get(category, {'red': 0.3, 'green': 0.3, 'blue': 0.5})
        
        sheet_id = worksheet.id
        requests = [
            # ãƒ˜ãƒƒãƒ€ãƒ¼æ›¸å¼
            {'repeatCell': {
                'range': {'sheetId': sheet_id, 'startRowIndex': 0, 'endRowIndex': 1, 'startColumnIndex': 0, 'endColumnIndex': 8},
                'cell': {'userEnteredFormat': {
                    'backgroundColor': bg_color,
                    'textFormat': {'foregroundColor': {'red': 1, 'green': 1, 'blue': 1}, 'bold': True, 'fontSize': 11},
                    'horizontalAlignment': 'CENTER', 'verticalAlignment': 'MIDDLE'
                }},
                'fields': 'userEnteredFormat(backgroundColor,textFormat,horizontalAlignment,verticalAlignment)'
            }},
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œé«˜ã•
            {'updateDimensionProperties': {
                'range': {'sheetId': sheet_id, 'dimension': 'ROWS', 'startIndex': 0, 'endIndex': 1},
                'properties': {'pixelSize': 40}, 'fields': 'pixelSize'}},
            # åˆ—å¹…
            {'updateDimensionProperties': {'range': {'sheetId': sheet_id, 'dimension': 'COLUMNS', 'startIndex': 0, 'endIndex': 1}, 'properties': {'pixelSize': 45}, 'fields': 'pixelSize'}},   # No.
            {'updateDimensionProperties': {'range': {'sheetId': sheet_id, 'dimension': 'COLUMNS', 'startIndex': 1, 'endIndex': 2}, 'properties': {'pixelSize': 100}, 'fields': 'pixelSize'}},  # ã‚½ãƒ¼ã‚¹
            {'updateDimensionProperties': {'range': {'sheetId': sheet_id, 'dimension': 'COLUMNS', 'startIndex': 2, 'endIndex': 3}, 'properties': {'pixelSize': 300}, 'fields': 'pixelSize'}},  # ã‚¿ã‚¤ãƒˆãƒ«
            {'updateDimensionProperties': {'range': {'sheetId': sheet_id, 'dimension': 'COLUMNS', 'startIndex': 3, 'endIndex': 4}, 'properties': {'pixelSize': 100}, 'fields': 'pixelSize'}},  # æ—¥ä»˜
            {'updateDimensionProperties': {'range': {'sheetId': sheet_id, 'dimension': 'COLUMNS', 'startIndex': 4, 'endIndex': 5}, 'properties': {'pixelSize': 140}, 'fields': 'pixelSize'}},  # ã‚¿ã‚°
            {'updateDimensionProperties': {'range': {'sheetId': sheet_id, 'dimension': 'COLUMNS', 'startIndex': 5, 'endIndex': 6}, 'properties': {'pixelSize': 70}, 'fields': 'pixelSize'}},   # é‡è¦åº¦
            {'updateDimensionProperties': {'range': {'sheetId': sheet_id, 'dimension': 'COLUMNS', 'startIndex': 6, 'endIndex': 7}, 'properties': {'pixelSize': 500}, 'fields': 'pixelSize'}},  # è¦ç´„
            {'updateDimensionProperties': {'range': {'sheetId': sheet_id, 'dimension': 'COLUMNS', 'startIndex': 7, 'endIndex': 8}, 'properties': {'pixelSize': 100}, 'fields': 'pixelSize'}},  # URL
            # ãƒ˜ãƒƒãƒ€ãƒ¼å›ºå®š
            {'updateSheetProperties': {
                'properties': {'sheetId': sheet_id, 'gridProperties': {'frozenRowCount': 1}},
                'fields': 'gridProperties.frozenRowCount'
            }}
        ]
        self._execute_batch_update_with_retry(requests)
    
    def _update_worksheet_with_retry(self, worksheet, values, range_name, max_retries=3):
        """worksheet.updateã‚’ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§å®Ÿè¡Œï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼‰"""
        for attempt in range(max_retries):
            try:
                worksheet.update(values=values, range_name=range_name)
                time.sleep(0.2)  # æˆåŠŸå¾Œã‚‚å°‘ã—å¾…æ©Ÿ
                return True
            except Exception as e:
                if "429" in str(e) or "Quota exceeded" in str(e):
                    wait_time = (attempt + 1) * 3  # 3ç§’ã€6ç§’ã€9ç§’ã¨å¢—ã‚„ã™
                    print(f"      âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã€‚{wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                    logger.warning(f"Google Sheets APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã€‚{wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ (è©¦è¡Œ {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    if attempt == max_retries - 1:
                        # ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ã«é”ã—ãŸå ´åˆã€ã•ã‚‰ã«é•·ãå¾…æ©Ÿã—ã¦æœ€å¾Œã®è©¦è¡Œ
                        print(f"      âš ï¸ ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ã•ã‚‰ã«60ç§’å¾…æ©Ÿã—ã¦æœ€å¾Œã®è©¦è¡Œ...")
                        logger.warning("ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ã•ã‚‰ã«60ç§’å¾…æ©Ÿã—ã¦æœ€å¾Œã®è©¦è¡Œ")
                        time.sleep(60)
                        try:
                            worksheet.update(values=values, range_name=range_name)
                            time.sleep(0.2)
                            print(f"      âœ… æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤æˆåŠŸ")
                            logger.info("æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤æˆåŠŸ")
                            return True
                        except Exception as final_error:
                            log_exception(logger, final_error, "æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤å¤±æ•—")
                            print(f"      âš ï¸ æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤ã‚‚å¤±æ•—ã€‚ã“ã®æ“ä½œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                            return False
                else:
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã¯è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¦å†ç™ºç”Ÿ
                    log_exception(logger, e, "worksheet.updateã‚¨ãƒ©ãƒ¼")
                    raise
        return False
    
    def _execute_batch_update_with_retry(self, requests, max_retries=3):
        """ãƒãƒƒãƒæ›´æ–°ã‚’ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§å®Ÿè¡Œï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼‰"""
        for attempt in range(max_retries):
            try:
                self.spreadsheet.batch_update({'requests': requests})
                time.sleep(0.2)  # æˆåŠŸå¾Œã‚‚å°‘ã—å¾…æ©Ÿ
                return True
            except Exception as e:
                if "429" in str(e) or "Quota exceeded" in str(e):
                    wait_time = (attempt + 1) * 3  # 3ç§’ã€6ç§’ã€9ç§’ã¨å¢—ã‚„ã™
                    print(f"      âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã€‚{wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                    logger.warning(f"Google Sheets APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ï¼ˆãƒãƒƒãƒæ›´æ–°ï¼‰ã€‚{wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ (è©¦è¡Œ {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    if attempt == max_retries - 1:
                        # ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ã«é”ã—ãŸå ´åˆã€ã•ã‚‰ã«é•·ãå¾…æ©Ÿã—ã¦æœ€å¾Œã®è©¦è¡Œ
                        print(f"      âš ï¸ ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ã•ã‚‰ã«60ç§’å¾…æ©Ÿã—ã¦æœ€å¾Œã®è©¦è¡Œ...")
                        logger.warning("ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ã«é”ã—ã¾ã—ãŸï¼ˆãƒãƒƒãƒæ›´æ–°ï¼‰ã€‚ã•ã‚‰ã«60ç§’å¾…æ©Ÿã—ã¦æœ€å¾Œã®è©¦è¡Œ")
                        time.sleep(60)
                        try:
                            self.spreadsheet.batch_update({'requests': requests})
                            time.sleep(0.2)
                            print(f"      âœ… æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤æˆåŠŸ")
                            logger.info("æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤æˆåŠŸï¼ˆãƒãƒƒãƒæ›´æ–°ï¼‰")
                            return True
                        except Exception as final_error:
                            log_exception(logger, final_error, "æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤å¤±æ•—ï¼ˆãƒãƒƒãƒæ›´æ–°ï¼‰")
                            print(f"      âš ï¸ æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤ã‚‚å¤±æ•—ã€‚ã“ã®æ“ä½œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                            return False
                else:
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã¯è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¦å†ç™ºç”Ÿ
                    log_exception(logger, e, "batch_updateã‚¨ãƒ©ãƒ¼")
                    raise
        return False
    
    def _normalize_url(self, url: str) -> str:
        """URLã‚’æ­£è¦åŒ–ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
        if not url:
            return ""
        
        # HYPERLINKé–¢æ•°ã‹ã‚‰å®Ÿéš›ã®URLã‚’æŠ½å‡º
        if url.startswith('=HYPERLINK') or url.startswith('=hyperlink') or 'HYPERLINK' in url.upper():
            match = re.search(r'HYPERLINK\("([^"]+)"', url, re.IGNORECASE)
            if match:
                url = match.group(1)
        
        # URLã‚’æ­£è¦åŒ–
        url = url.strip()
        
        # "è¨˜äº‹ã‚’é–‹ã"ãªã©ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯é™¤å¤–
        if url == "è¨˜äº‹ã‚’é–‹ã" or url == "Open Article":
            return ""
        
        # æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
        url = url.rstrip('/')
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤ï¼ˆè¨˜äº‹URLã¯é€šå¸¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸è¦ï¼‰
        if '?' in url:
            base, params = url.split('?', 1)
            # é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¾‹: id, article_idï¼‰ãŒã‚ã‚‹å ´åˆã¯ä¿æŒ
            if 'id=' in params.lower() or 'article_id=' in params.lower():
                # IDãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ä¿æŒ
                param_dict = {}
                for param in params.split('&'):
                    if '=' in param:
                        key, value = param.split('=', 1)
                        if key.lower() in ['id', 'article_id', 'article']:
                            param_dict[key] = value
                if param_dict:
                    sorted_params = '&'.join([f"{k}={v}" for k, v in sorted(param_dict.items())])
                    url = f"{base}?{sorted_params}"
                else:
                    url = base
            else:
                url = base
        
        return url
    
    def _normalize_title(self, title: str) -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
        if not title:
            return ""
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ï¼ˆç©ºç™½ã‚’çµ±ä¸€ã€å¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ã€ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»ï¼‰
        normalized = title.lower().replace(" ", "").replace("ã€€", "").replace("ã€", "").replace("ï¼Œ", "")
        normalized = normalized.replace("ãƒ»", "").replace("ãƒ¼", "").replace("-", "").replace("â€•", "")
        return normalized
    
    def _load_existing_urls(self):
        """å…¨ã‚·ãƒ¼ãƒˆã‹ã‚‰æ—¢å­˜URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆæ­£è¦åŒ–ã—ã¦ä¿å­˜ï¼‰"""
        total = 0
        self.existing_urls = set()
        self.existing_titles = set()
        self.normalized_urls = {}
        self.normalized_titles = {}
        
        for category, ws in self.worksheets.items():
            try:
                all_values = ws.get_all_values()
                for row in all_values[1:]:
                    if len(row) >= 3:
                        # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆCåˆ—ï¼‰ã‚’ä¿å­˜
                        title = row[2] if len(row) > 2 else ""
                        if title:
                            self.existing_titles.add(title)
                            normalized_title = self._normalize_title(title)
                            if normalized_title:
                                self.normalized_titles[normalized_title] = title
                        
                        # URLï¼ˆHåˆ—ã¾ãŸã¯Iåˆ—ï¼‰ã‚’ä¿å­˜
                        url = ""
                        if len(row) > 8 and row[8] and row[8].startswith("http"):
                            url = row[8]
                        elif len(row) > 7 and row[7] and row[7].startswith("http"):
                            url = row[7]
                        
                        if url:
                            self.existing_urls.add(url)
                            normalized_url = self._normalize_url(url)
                            if normalized_url:
                                self.normalized_urls[normalized_url] = url
                        
                        total += 1
            except Exception as e:
                log_exception(logger, e, f"ã‚·ãƒ¼ãƒˆã€Œ{category}ã€ã‹ã‚‰æ—¢å­˜URLèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼")
        if total > 0:
            print(f"   æ—¢å­˜è¨˜äº‹æ•°: {total}ä»¶ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã«æ­£è¦åŒ–æ¸ˆã¿ï¼‰")
            logger.info(f"æ—¢å­˜è¨˜äº‹æ•°: {total}ä»¶ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã«æ­£è¦åŒ–æ¸ˆã¿ï¼‰")
    
    def is_duplicate(self, url: str, title: str = "") -> bool:
        """URLã¾ãŸã¯ã‚¿ã‚¤ãƒˆãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦åŒ–ã—ã¦æ¯”è¼ƒï¼‰"""
        # URLã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦åŒ–ï¼‰
        if url:
            normalized_url = self._normalize_url(url)
            if normalized_url and normalized_url in self.normalized_urls:
                existing_url = self.normalized_urls[normalized_url]
                logger.debug(f"URLé‡è¤‡ã‚’æ¤œå‡º: {url} (æ—¢å­˜: {existing_url})")
                return True
            # æ­£è¦åŒ–å‰ã®URLã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆå¿µã®ãŸã‚ï¼‰
            if url in self.existing_urls:
                logger.debug(f"URLé‡è¤‡ã‚’æ¤œå‡ºï¼ˆæ­£è¦åŒ–å‰ï¼‰: {url}")
                return True
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦åŒ–ï¼‰
        if title:
            normalized_title = self._normalize_title(title)
            if normalized_title and normalized_title in self.normalized_titles:
                existing_title = self.normalized_titles[normalized_title]
                logger.debug(f"ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ã‚’æ¤œå‡º: {title} (æ—¢å­˜: {existing_title})")
                return True
            # æ­£è¦åŒ–å‰ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆå¿µã®ãŸã‚ï¼‰
            if title in self.existing_titles:
                logger.debug(f"ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ã‚’æ¤œå‡ºï¼ˆæ­£è¦åŒ–å‰ï¼‰: {title}")
                return True
        
        return False
    
    def _get_category(self, article: dict) -> str:
        """è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®šï¼ˆè¤‡æ•°ã‚«ãƒ†ã‚´ãƒªå¯¾å¿œï¼‰"""
        text = f"{article.get('title', '')} {' '.join(article.get('tags', []))} {article.get('content', '')[:500]}"
        text_lower = text.lower()
        
        # ãƒãƒƒãƒã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’å…¨ã¦åé›†
        matched_categories = []
        
        # å„ªå…ˆé †ä½é †ã«ãƒã‚§ãƒƒã‚¯ï¼ˆä¼æ¥­åŠ¹ç‡åŒ–ã€DXãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã‚’å„ªå…ˆï¼‰
        category_priority = [
            "ä¼æ¥­åŠ¹ç‡åŒ–",
            "DXãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–",
            "ä¼æ¥­å°å…¥",
            "AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
        ]
        
        for category in category_priority:
            if category in KEYWORD_CATEGORIES:
                keywords = KEYWORD_CATEGORIES[category]
                if any(kw.lower() in text_lower for kw in keywords):
                    matched_categories.append(category)
        
        # ãƒãƒƒãƒã—ãŸã‚«ãƒ†ã‚´ãƒªãŒãªã„å ´åˆã¯ã€Œãã®ä»–ã€
        if not matched_categories:
            return "ãã®ä»–"
        
        # è¤‡æ•°ãƒãƒƒãƒã—ãŸå ´åˆã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¿”ã™
        # ãŸã ã—ã€ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªï¼ˆæœ€åˆã«ãƒãƒƒãƒã—ãŸã‚‚ã®ï¼‰ã‚’æœ€åˆã«é…ç½®
        return ", ".join(matched_categories)
    
    def add_article(self, article: dict, summary: str, score: int = 3) -> bool:
        """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚·ãƒ¼ãƒˆã«è¿½åŠ ï¼ˆè¤‡æ•°ã‚«ãƒ†ã‚´ãƒªå¯¾å¿œï¼‰"""
        url = article.get("url", "")
        title = article.get("title", "")
        
        if self.is_duplicate(url, title):
            return False
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šï¼ˆè¤‡æ•°ã‚«ãƒ†ã‚´ãƒªã®å ´åˆã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰
        category_str = self._get_category(article)
        categories = [cat.strip() for cat in category_str.split(",")]
        
        # ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªï¼ˆæœ€åˆã®ã‚«ãƒ†ã‚´ãƒªï¼‰ã®ã‚·ãƒ¼ãƒˆã«è¿½åŠ 
        main_category = categories[0] if categories else "ãã®ä»–"
        
        # ã‚·ãƒ¼ãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        if main_category not in self.worksheets:
            try:
                ws = self.spreadsheet.worksheet(main_category)
                self.worksheets[main_category] = ws
            except gspread.WorksheetNotFound:
                # æ–°ã—ã„ã‚·ãƒ¼ãƒˆã‚’ä½œæˆ
                ws = self.spreadsheet.add_worksheet(title=main_category, rows=200, cols=10)
                self.worksheets[main_category] = ws
                self._setup_sheet_headers(ws, main_category)
                print(f"   ğŸ“„ æ–°è¦ã‚·ãƒ¼ãƒˆã€Œ{main_category}ã€ã‚’ä½œæˆã—ã¾ã—ãŸ")
        
        worksheet = self.worksheets.get(main_category, self.worksheets.get("ãã®ä»–"))
        
        row_num = len(worksheet.col_values(1)) + 1
        article_no = row_num - 1
        
        # ã‚·ãƒ¼ãƒˆã®è¡Œæ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯è¿½åŠ 
        current_row_count = worksheet.row_count
        if row_num > current_row_count:
            rows_to_add = row_num - current_row_count + 10  # ä½™è£•ã‚’æŒã£ã¦10è¡Œè¿½åŠ 
            try:
                worksheet.add_rows(rows_to_add)
                print(f"   ğŸ“ ã‚·ãƒ¼ãƒˆã€Œ{main_category}ã€ã«è¡Œã‚’{rows_to_add}è¡Œè¿½åŠ ã—ã¾ã—ãŸï¼ˆç¾åœ¨: {current_row_count}è¡Œ â†’ {current_row_count + rows_to_add}è¡Œï¼‰")
                logger.info(f"ã‚·ãƒ¼ãƒˆã€Œ{main_category}ã€ã«è¡Œã‚’{rows_to_add}è¡Œè¿½åŠ ã—ã¾ã—ãŸ")
            except Exception as e:
                log_exception(logger, e, f"ã‚·ãƒ¼ãƒˆã€Œ{main_category}ã€ã¸ã®è¡Œè¿½åŠ ã‚¨ãƒ©ãƒ¼")
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶šè¡Œï¼ˆæ—¢å­˜ã®è¡Œæ•°å†…ã§è©¦è¡Œï¼‰
        
        # é‡è¦åº¦ã‚’æ˜Ÿã§è¡¨ç¤º
        score_display = "â­" * score + "â˜†" * (5 - score)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ï¼ˆIåˆ—ã«å®ŸURLã‚’ä¿å­˜ï¼‰
        # ã‚«ãƒ†ã‚´ãƒªåˆ—ã«ã¯è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ä¿å­˜
        data = [
            article_no,
            article.get("source", ""),
            article.get("title", ""),
            article.get("date", ""),
            ", ".join(article.get("tags", [])),
            score_display,  # é‡è¦åº¦
            summary,
            "",  # URLåˆ—ã¯å¾Œã§ãƒã‚¤ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯è¨­å®š
            url,   # å®ŸURLï¼ˆWebã‚¢ãƒ—ãƒªç”¨ï¼‰
            category_str  # ã‚«ãƒ†ã‚´ãƒªï¼ˆè¤‡æ•°ã®å ´åˆã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰
        ]
        
        # ãƒ‡ãƒ¼ã‚¿æ›´æ–°ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        self._update_worksheet_with_retry(worksheet, values=[data], range_name=f'A{row_num}:J{row_num}')
        
        # æ—¢å­˜URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã«è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
        if url:
            self.existing_urls.add(url)
            normalized_url = self._normalize_url(url)
            if normalized_url:
                self.normalized_urls[normalized_url] = url
        
        if title:
            self.existing_titles.add(title)
            normalized_title = self._normalize_title(title)
            if normalized_title:
                self.normalized_titles[normalized_title] = title
        
        # ã‚¹ã‚³ã‚¢ã«å¿œã˜ãŸèƒŒæ™¯è‰²
        score_colors = {
            5: {'red': 1.0, 'green': 0.9, 'blue': 0.6},    # é‡‘è‰²
            4: {'red': 0.9, 'green': 0.95, 'blue': 0.7},   # è–„ç·‘
            3: {'red': 1.0, 'green': 1.0, 'blue': 1.0},    # ç™½
            2: {'red': 0.95, 'green': 0.95, 'blue': 0.95}, # è–„ç°
            1: {'red': 0.9, 'green': 0.9, 'blue': 0.9},    # ç°è‰²
        }
        score_bg = score_colors.get(score, score_colors[3])
        
        # è¡Œã®æ›¸å¼è¨­å®š + URLã‚’ãƒã‚¤ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯ã¨ã—ã¦è¨­å®š
        sheet_id = worksheet.id
        requests = [
                # è¡Œã®æ›¸å¼è¨­å®š
                {'repeatCell': {
                    'range': {'sheetId': sheet_id, 'startRowIndex': row_num - 1, 'endRowIndex': row_num, 'startColumnIndex': 0, 'endColumnIndex': 7},
                    'cell': {'userEnteredFormat': {'wrapStrategy': 'WRAP', 'verticalAlignment': 'TOP', 'textFormat': {'fontSize': 10}}},
                    'fields': 'userEnteredFormat(wrapStrategy,verticalAlignment,textFormat)'
                }},
                # é‡è¦åº¦ã‚»ãƒ«ã®æ›¸å¼ï¼ˆä¸­å¤®æƒãˆ + èƒŒæ™¯è‰²ï¼‰
                {'repeatCell': {
                    'range': {'sheetId': sheet_id, 'startRowIndex': row_num - 1, 'endRowIndex': row_num, 'startColumnIndex': 5, 'endColumnIndex': 6},
                    'cell': {'userEnteredFormat': {
                        'horizontalAlignment': 'CENTER',
                        'verticalAlignment': 'MIDDLE',
                        'backgroundColor': score_bg
                    }},
                    'fields': 'userEnteredFormat(horizontalAlignment,verticalAlignment,backgroundColor)'
                }},
                # è¡Œã®é«˜ã•
                {'updateDimensionProperties': {
                    'range': {'sheetId': sheet_id, 'dimension': 'ROWS', 'startIndex': row_num - 1, 'endIndex': row_num},
                    'properties': {'pixelSize': 120}, 'fields': 'pixelSize'}},
                # URLã‚’ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªãƒã‚¤ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯ã¨ã—ã¦è¨­å®š
                {'updateCells': {
                    'range': {'sheetId': sheet_id, 'startRowIndex': row_num - 1, 'endRowIndex': row_num, 'startColumnIndex': 7, 'endColumnIndex': 8},
                    'rows': [{
                        'values': [{
                            'userEnteredValue': {'stringValue': 'ğŸ”— è¨˜äº‹ã‚’é–‹ã'},
                            'textFormatRuns': [{
                                'startIndex': 0,
                                'format': {'link': {'uri': url}, 'foregroundColor': {'red': 0.06, 'green': 0.46, 'blue': 0.88}}
                            }]
                        }]
                    }],
                    'fields': 'userEnteredValue,textFormatRuns'
                }}
            ]
        self._execute_batch_update_with_retry(requests)
        
        return True
    
    def get_spreadsheet_url(self) -> str:
        """ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URLã‚’å–å¾—"""
        return self.spreadsheet.url
    
    def get_total_article_count(self) -> int:
        """ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ç·è¨˜äº‹æ•°ã‚’å–å¾—"""
        total = 0
        for category, ws in self.worksheets.items():
            try:
                all_values = ws.get_all_values()
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã„ãŸæœ‰åŠ¹ãªè¨˜äº‹æ•°ï¼ˆ7åˆ—ä»¥ä¸Šã‚ã‚‹è¡Œï¼‰
                for row in all_values[1:]:
                    if len(row) >= 7 and row[2]:  # ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹è¡Œ
                        total += 1
            except Exception as e:
                log_exception(logger, e, f"ã‚·ãƒ¼ãƒˆã€Œ{category}ã€ã®è¨˜äº‹æ•°å–å¾—ã‚¨ãƒ©ãƒ¼")
        return total
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
        if not date_str or not date_str.strip():
            return None
        
        date_str = date_str.strip()
        date_formats = [
            '%Y-%m-%d',           # 2024-12-12
            '%Y/%m/%d',           # 2024/12/12
            '%Yå¹´%mæœˆ%dæ—¥',        # 2024å¹´12æœˆ12æ—¥
            '%Y-%m-%d %H:%M:%S',  # 2024-12-12 12:00:00
            '%Y/%m/%d %H:%M:%S',  # 2024/12/12 12:00:00
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.split()[0], fmt.split()[0])
            except (ValueError, IndexError):
                continue
        
        # æ—¥ä»˜å½¢å¼ãŒä¸æ˜ãªå ´åˆã¯Noneã‚’è¿”ã™
        return None
    
    def delete_old_articles(self, retention_days: int = 30) -> int:
        """ä¸€å®šæœŸé–“çµŒéã—ãŸå¤ã„è¨˜äº‹ã‚’å‰Šé™¤ï¼ˆâ˜…5ã®è¨˜äº‹ã¯æ°¸ä¹…ä¿å­˜ï¼‰"""
        print(f"\nğŸ—‘ï¸  {retention_days}æ—¥ä»¥ä¸ŠçµŒéã—ãŸå¤ã„è¨˜äº‹ã‚’å‰Šé™¤ä¸­...ï¼ˆâ˜…5ã¯æ°¸ä¹…ä¿å­˜ï¼‰")
        logger.info(f"å¤ã„è¨˜äº‹ã®å‰Šé™¤ã‚’é–‹å§‹ï¼ˆä¿æŒæœŸé–“: {retention_days}æ—¥ã€â˜…5ã¯æ°¸ä¹…ä¿å­˜ï¼‰")
        
        total_deleted = 0
        preserved_count = 0  # â˜…5ã§ä¿å­˜ã•ã‚ŒãŸè¨˜äº‹æ•°
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        for category, ws in self.worksheets.items():
            try:
                all_values = ws.get_all_values()
                if len(all_values) <= 1:
                    continue
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã
                data_rows = all_values[1:]
                rows_to_delete = []
                
                for row_index, row in enumerate(data_rows, start=2):  # 2è¡Œç›®ã‹ã‚‰ï¼ˆ1è¡Œç›®ã¯ãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰
                    if len(row) < 4:
                        continue
                    
                    date_str = row[3] if len(row) > 3 else ""  # æ—¥ä»˜åˆ—ï¼ˆ4åˆ—ç›®ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹3ï¼‰
                    title = row[2] if len(row) > 2 else ""      # ã‚¿ã‚¤ãƒˆãƒ«åˆ—ï¼ˆ3åˆ—ç›®ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹2ï¼‰
                    score_str = row[5] if len(row) > 5 else ""  # é‡è¦åº¦åˆ—ï¼ˆ6åˆ—ç›®ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹5ï¼‰
                    
                    if not date_str or not title:
                        continue
                    
                    # â˜…5ã®è¨˜äº‹ã¯æ°¸ä¹…ä¿å­˜ï¼ˆå‰Šé™¤å¯¾è±¡å¤–ï¼‰
                    # ã‚¹ã‚³ã‚¢åˆ—ã‹ã‚‰â­ã®æ•°ã‚’æ•°ãˆã¦ã€5å€‹ä»¥ä¸Šã‚ã‚Œã°æ°¸ä¹…ä¿å­˜
                    if score_str:
                        star_count = score_str.count("â­")
                        if star_count >= 5:
                            preserved_count += 1
                            logger.debug(f"æ°¸ä¹…ä¿å­˜: [{category}] è¡Œ{row_index} - â˜…5è¨˜äº‹: {title[:50]}...")
                            continue
                    
                    # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
                    article_date = self._parse_date(date_str)
                    if not article_date:
                        # æ—¥ä»˜ãŒãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‰Šé™¤ã—ãªã„ï¼‰
                        continue
                    
                    # ä¿æŒæœŸé–“ã‚’è¶…ãˆã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if article_date < cutoff_date:
                        rows_to_delete.append(row_index)
                        print(f"   ğŸ—‘ï¸  [{category}] è¡Œ{row_index}: {title[:50]}... (æ—¥ä»˜: {date_str})")
                        logger.info(f"å‰Šé™¤å¯¾è±¡: [{category}] è¡Œ{row_index} - {title[:50]}... (æ—¥ä»˜: {date_str})")
                
                # è¡Œã‚’å‰Šé™¤ï¼ˆå¾Œã‚ã‹ã‚‰å‰Šé™¤ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰
                if rows_to_delete:
                    rows_to_delete.sort(reverse=True)
                    for row_index in rows_to_delete:
                        try:
                            ws.delete_rows(row_index)
                            total_deleted += 1
                            logger.debug(f"è¡Œ{row_index}ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                        except Exception as e:
                            log_exception(logger, e, f"è¡Œ{row_index}ã®å‰Šé™¤ã‚¨ãƒ©ãƒ¼")
                            print(f"      âš ï¸ è¡Œ{row_index}ã®å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
                
            except Exception as e:
                log_exception(logger, e, f"ã‚·ãƒ¼ãƒˆã€Œ{category}ã€ã®å¤ã„è¨˜äº‹å‰Šé™¤ã‚¨ãƒ©ãƒ¼")
                print(f"   âš ï¸ ã‚·ãƒ¼ãƒˆã€Œ{category}ã€ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        if total_deleted > 0:
            print(f"   âœ… {total_deleted}ä»¶ã®å¤ã„è¨˜äº‹ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            logger.info(f"å¤ã„è¨˜äº‹ã®å‰Šé™¤å®Œäº†: {total_deleted}ä»¶")
        else:
            print(f"   âœ… å‰Šé™¤å¯¾è±¡ã®è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            logger.info("å‰Šé™¤å¯¾è±¡ã®è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        if preserved_count > 0:
            print(f"   ğŸ’ {preserved_count}ä»¶ã®â˜…5è¨˜äº‹ã‚’æ°¸ä¹…ä¿å­˜ã—ã¾ã—ãŸ")
            logger.info(f"â˜…5è¨˜äº‹ã®æ°¸ä¹…ä¿å­˜: {preserved_count}ä»¶")
        
        return total_deleted


def filter_by_keywords(articles: list[dict]) -> list[dict]:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆç·©å’Œç‰ˆï¼šã‚ˆã‚Šå¤šãã®è¨˜äº‹ã‚’é€šã™ï¼‰"""
    filtered = []
    for article in articles:
        # ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚¿ã‚°ã€æœ¬æ–‡ã®æœ€åˆã®1000æ–‡å­—ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç¯„å›²ã‚’æ‹¡å¤§ï¼‰
        text = f"{article.get('title', '')} {' '.join(article.get('tags', []))} {article.get('content', '')[:1000]}"
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ï¼ˆéƒ¨åˆ†ä¸€è‡´ã§ã‚‚OKï¼‰
        if any(keyword.lower() in text.lower() for keyword in KEYWORDS):
            filtered.append(article)
    return filtered


def parse_article_date(date_str: str) -> Optional[datetime]:
    """è¨˜äº‹ã®æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    if not date_str:
        return None
    
    # æ§˜ã€…ãªæ—¥ä»˜å½¢å¼ã«å¯¾å¿œ
    date_formats = [
        "%Y/%m/%d",
        "%Yå¹´%mæœˆ%dæ—¥",
        "%Y-%m-%d",
        "%Y/%m/%d %H:%M",
        "%Y-%m-%d %H:%M:%S",
        "%Yå¹´%mæœˆ%dæ—¥ %H:%M",
    ]
    
    for fmt in date_formats:
        try:
            return datetime.strptime(date_str.strip(), fmt)
        except ValueError:
            continue
    
    # æ­£è¦è¡¨ç¾ã§æŠ½å‡ºã‚’è©¦ã¿ã‚‹
    date_match = re.search(r'(\d{4})[/å¹´-](\d{1,2})[/æœˆ-](\d{1,2})', date_str)
    if date_match:
        year, month, day = date_match.groups()
        try:
            return datetime(int(year), int(month), int(day))
        except ValueError:
            pass
    
    return None


def sort_articles_by_date(articles: list[dict], reverse: bool = True) -> list[dict]:
    """è¨˜äº‹ã‚’æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰"""
    def get_sort_key(article: dict) -> datetime:
        date_str = article.get("date", "")
        parsed_date = parse_article_date(date_str)
        # æ—¥ä»˜ãŒè§£æã§ããªã„å ´åˆã¯ã€éå¸¸ã«å¤ã„æ—¥ä»˜ã¨ã—ã¦æ‰±ã†
        if parsed_date is None:
            return datetime(1900, 1, 1)
        return parsed_date
    
    return sorted(articles, key=get_sort_key, reverse=reverse)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 70)
    print("ğŸ¤– ãƒãƒ«ãƒã‚µã‚¤ãƒˆå¯¾å¿œ AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° & è¦ç´„ãƒ„ãƒ¼ãƒ«")
    print("   ğŸ“° å¯¾å¿œã‚µã‚¤ãƒˆ: Ledge.ai, AINOW, PR TIMES, ITmedia AI+")
    print("=" * 70)
    
    logger.info("=" * 70)
    logger.info("ãƒãƒ«ãƒã‚µã‚¤ãƒˆå¯¾å¿œ AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° & è¦ç´„ãƒ„ãƒ¼ãƒ«ã‚’é–‹å§‹")
    logger.info("å¯¾å¿œã‚µã‚¤ãƒˆ: Ledge.ai, AINOW, PR TIMES, ITmedia AI+")
    logger.info("=" * 70)
    
    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        error_msg = "OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
        logger.error(error_msg)
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {error_msg}")
        return
    
    print(f"âœ… OpenAI APIã‚­ãƒ¼: è¨­å®šæ¸ˆã¿")
    logger.info("OpenAI APIã‚­ãƒ¼: è¨­å®šæ¸ˆã¿")
    
    # Google SheetsåˆæœŸåŒ–
    try:
        exporter = GoogleSheetsExporter()
    except Exception as e:
        log_exception(logger, e, "Google SheetsåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼")
        print(f"âŒ Google Sheetsèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒªã‚¹ãƒˆ
    scrapers = [
        LedgeAiScraper(),
        AINowScraper(),
        PRTimesScraper(),
        ITmediaAiPlusScraper(),
    ]
    
    # å…¨ã‚µã‚¤ãƒˆã‹ã‚‰è¨˜äº‹ã‚’åé›†
    all_articles = []
    for scraper in scrapers:
        try:
            # å„ã‚µã‚¤ãƒˆã‹ã‚‰æœ€å¤§15ä»¶ã®è¨˜äº‹ã‚’å–å¾—
            articles = scraper.scrape(max_pages=10, max_articles=15)
            print(f"   ğŸ“° {scraper.SITE_NAME}: {len(articles)}ä»¶å–å¾—")
            logger.info(f"{scraper.SITE_NAME}: {len(articles)}ä»¶å–å¾—")
            all_articles.extend(articles)
        except Exception as e:
            log_exception(logger, e, f"{scraper.SITE_NAME} ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼")
            print(f"âš ï¸ {scraper.SITE_NAME} ã§ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"\nğŸ“Š å…¨ã‚µã‚¤ãƒˆåˆè¨ˆ: {len(all_articles)}ä»¶")
    
    # ã‚µã‚¤ãƒˆåˆ¥å†…è¨³ã‚’è¡¨ç¤º
    source_counts = {}
    for article in all_articles:
        source = article.get("source", "Unknown")
        source_counts[source] = source_counts.get(source, 0) + 1
    print("   ğŸ“‚ ã‚µã‚¤ãƒˆåˆ¥å†…è¨³:")
    for source, count in source_counts.items():
        print(f"      - {source}: {count}ä»¶")
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    print("\nğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")
    filtered_articles = filter_by_keywords(all_articles)
    print(f"   ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(filtered_articles)}ä»¶")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ã‚µã‚¤ãƒˆåˆ¥å†…è¨³
    filtered_source_counts = {}
    for article in filtered_articles:
        source = article.get("source", "Unknown")
        filtered_source_counts[source] = filtered_source_counts.get(source, 0) + 1
    print("   ğŸ“‚ ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ã‚µã‚¤ãƒˆåˆ¥å†…è¨³:")
    for source, count in filtered_source_counts.items():
        print(f"      - {source}: {count}ä»¶")
    
    # å¤ã„è¨˜äº‹ã®è‡ªå‹•å‰Šé™¤ï¼ˆæ–°è¦è¨˜äº‹è¿½åŠ å‰ã«å®Ÿè¡Œï¼‰
    deleted_count = exporter.delete_old_articles(ARTICLE_RETENTION_DAYS)
    if deleted_count > 0:
        print(f"   ğŸ“Š å‰Šé™¤å¾Œã®ç·è¨˜äº‹æ•°: {exporter.get_total_article_count()}ä»¶")
    
    # é‡è¤‡é™¤å¤–
    new_articles = [a for a in filtered_articles if not exporter.is_duplicate(a.get("url", ""), a.get("title", ""))]
    skipped = len(filtered_articles) - len(new_articles)
    
    if skipped > 0:
        print(f"   â­ï¸ æ—¢å­˜è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—: {skipped}ä»¶")
    
    if not new_articles:
        print("\nâœ… æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        print(f"ğŸ”— ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ: {exporter.get_spreadsheet_url()}")
        return
    
    print(f"   ğŸ†• æ–°è¦è¨˜äº‹: {len(new_articles)}ä»¶")
    
    # PR TIMESã®è¨˜äº‹ã‚’æœ€å¤§4ã¤ã¾ã§ã«åˆ¶é™
    PR_TIMES_MAX = 4
    pr_times_articles = [a for a in new_articles if a.get("source", "") == "PR TIMES"]
    other_articles = [a for a in new_articles if a.get("source", "") != "PR TIMES"]
    
    # PR TIMESã®è¨˜äº‹ã‚’å…ˆç€é †ã§æœ€å¤§4ã¤ã¾ã§é¸ã¶ï¼ˆæ—¥ä»˜é †ã§ã¯ãªã„ï¼‰
    pr_times_selected = pr_times_articles[:PR_TIMES_MAX]
    pr_times_skipped = len(pr_times_articles) - len(pr_times_selected)
    
    # ãã®ä»–ã®è¨˜äº‹ã‚’æ—¥ä»˜é †ï¼ˆæ–°ã—ã„é †ï¼‰ã«ã‚½ãƒ¼ãƒˆã—ã¦ã‹ã‚‰æœ€å¤§11ã¤é¸ã¶
    print(f"\nğŸ“… ãã®ä»–ã®è¨˜äº‹ã‚’æ—¥ä»˜é †ï¼ˆæ–°ã—ã„é †ï¼‰ã«ã‚½ãƒ¼ãƒˆä¸­...")
    other_articles_sorted = sort_articles_by_date(other_articles, reverse=True)
    other_selected = other_articles_sorted[:MAX_ARTICLES_PER_RUN - PR_TIMES_MAX]
    other_skipped = len(other_articles) - len(other_selected)
    
    # æ—¥ä»˜é †ã‚½ãƒ¼ãƒˆçµæœã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    if other_articles_sorted:
        print(f"   ğŸ“… æ—¥ä»˜é †ã‚½ãƒ¼ãƒˆçµæœï¼ˆä¸Šä½5ä»¶ï¼‰:")
        for i, article in enumerate(other_articles_sorted[:5], 1):
            date_str = article.get("date", "æ—¥ä»˜ä¸æ˜")
            title = article.get("title", "")[:40]
            print(f"      {i}. [{date_str}] {title}...")
    
    if pr_times_skipped > 0:
        print(f"   âš ï¸ PR TIMESã®è¨˜äº‹ã‚’{PR_TIMES_MAX}ä»¶ã«åˆ¶é™ï¼ˆ{pr_times_skipped}ä»¶ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        logger.info(f"PR TIMESã®è¨˜äº‹ã‚’{PR_TIMES_MAX}ä»¶ã«åˆ¶é™ï¼ˆ{pr_times_skipped}ä»¶ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    
    if other_skipped > 0:
        print(f"   âš ï¸ ãã®ä»–ã®è¨˜äº‹ã‚’{MAX_ARTICLES_PER_RUN - PR_TIMES_MAX}ä»¶ã«åˆ¶é™ï¼ˆ{other_skipped}ä»¶ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        logger.info(f"ãã®ä»–ã®è¨˜äº‹ã‚’{MAX_ARTICLES_PER_RUN - PR_TIMES_MAX}ä»¶ã«åˆ¶é™ï¼ˆ{other_skipped}ä»¶ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    
    # é¸ã‚“ã è¨˜äº‹ã‚’çµåˆï¼ˆã¾ãšå…¨è¨˜äº‹ã‚’è¦ç´„ç”Ÿæˆã—ã¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—ï¼‰
    articles_to_evaluate = pr_times_selected + other_selected
    print(f"   ğŸ“Œ ä»Šå›è©•ä¾¡ã™ã‚‹è¨˜äº‹: {len(articles_to_evaluate)}ä»¶ï¼ˆPR TIMES: {len(pr_times_selected)}ä»¶ã€ãã®ä»–: {len(other_selected)}ä»¶ï¼‰")
    
    # è¦ç´„ç”Ÿæˆ + é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆå…¨è¨˜äº‹ã‚’è©•ä¾¡ï¼‰
    summarizer = ArticleSummarizer(api_key)
    print("\nâœï¸ OpenAI APIã§è¦ç´„ & é‡è¦åº¦è©•ä¾¡ä¸­...")
    
    # è¨˜äº‹ã¨ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
    articles_with_scores = []
    
    for i, article in enumerate(articles_to_evaluate, 1):
        source = article.get('source', '')
        title = article.get('title', '')[:35]
        print(f"   [{i}/{len(articles_to_evaluate)}] [{source}] {title}...")
        logger.info(f"[{i}/{len(articles_to_evaluate)}] [{source}] {title[:50]}...")
        
        try:
            result = summarizer.summarize_and_score(article)
            summary = result["summary"]
            score = result["score"]
            
            print(f"      â†’ é‡è¦åº¦: {'â­' * score}")
            logger.info(f"é‡è¦åº¦: {'â­' * score}")
            
            # è¨˜äº‹ã€è¦ç´„ã€ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜
            articles_with_scores.append({
                'article': article,
                'summary': summary,
                'score': score
            })
        except Exception as e:
            log_exception(logger, e, f"è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {title[:50]}")
            print(f"      âš ï¸ è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        time.sleep(0.5)
    
    # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆï¼ˆã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ï¼‰
    articles_with_scores.sort(key=lambda x: x['score'], reverse=True)
    
    # PR TIMESã‚’æœ€å¤§4ã¤ã€ãã®ä»–ã‚’é‡è¦åº¦é †ã«é¸ã¶
    pr_times_final = []
    other_final = []
    
    for item in articles_with_scores:
        article = item['article']
        if article.get("source", "") == "PR TIMES":
            if len(pr_times_final) < PR_TIMES_MAX:
                pr_times_final.append(item)
        else:
            other_final.append(item)
    
    # ãã®ä»–ã®è¨˜äº‹ã‚’é‡è¦åº¦é †ã«æœ€å¤§11ã¤ã¾ã§é¸ã¶
    other_final = other_final[:MAX_ARTICLES_PER_RUN - PR_TIMES_MAX]
    
    # æœ€çµ‚çš„ã«åæ˜ ã™ã‚‹è¨˜äº‹ã‚’çµåˆï¼ˆPR TIMESã‚’å…ˆã«ã€ãã®ä»–ã‚’é‡è¦åº¦é †ã«ï¼‰
    articles_to_process = pr_times_final + other_final
    
    print(f"\nğŸ“Š é‡è¦åº¦é †ã«é¸å®šå®Œäº†:")
    print(f"   - PR TIMES: {len(pr_times_final)}ä»¶")
    print(f"   - ãã®ä»–ï¼ˆé‡è¦åº¦é †ï¼‰: {len(other_final)}ä»¶")
    print(f"   - åˆè¨ˆ: {len(articles_to_process)}ä»¶")
    logger.info(f"é‡è¦åº¦é †ã«é¸å®šå®Œäº†: PR TIMES {len(pr_times_final)}ä»¶ã€ãã®ä»– {len(other_final)}ä»¶")
    
    # è¨˜äº‹ã‚’åæ˜ 
    added = 0
    score_stats = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
    
    for item in articles_to_process:
        article = item['article']
        summary = item['summary']
        score = item['score']
        score_stats[score] += 1
        
        if exporter.add_article(article, summary, score):
            added += 1
            title = article.get('title', '')[:50]
            logger.info(f"è¨˜äº‹ã‚’è¿½åŠ ã—ã¾ã—ãŸ: {title}")
        else:
            title = article.get('title', '')[:50]
            logger.warning(f"è¨˜äº‹ã®è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆé‡è¤‡ã®å¯èƒ½æ€§ï¼‰: {title}")
    
    # ã‚µã‚¤ãƒˆåˆ¥é›†è¨ˆï¼ˆå‡¦ç†ã—ãŸè¨˜äº‹ã®ã¿ï¼‰
    source_counts = {}
    for item in articles_to_process:
        article = item['article']
        source = article.get('source', 'Unknown')
        source_counts[source] = source_counts.get(source, 0) + 1
    
    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ç·è¨˜äº‹æ•°ã‚’å–å¾—
    total_articles = exporter.get_total_article_count()
    
    # ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸè¨˜äº‹æ•°ã‚’è¨ˆç®—
    total_skipped = pr_times_skipped + other_skipped
    
    print("\n" + "=" * 70)
    print("ğŸ‰ å‡¦ç†å®Œäº†ï¼")
    print(f"   ğŸ“Š åé›†è¨˜äº‹æ•°: {len(filtered_articles)}ä»¶")
    print(f"   â­ï¸ æ—¢å­˜è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—: {skipped}ä»¶")
    if total_skipped > 0:
        print(f"   âš ï¸ è¿½åŠ æ•°åˆ¶é™ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—: {total_skipped}ä»¶ï¼ˆæ¬¡å›å®Ÿè¡Œæ™‚ã«å‡¦ç†ï¼‰")
        if pr_times_skipped > 0:
            print(f"      - PR TIMES: {pr_times_skipped}ä»¶")
        if other_skipped > 0:
            print(f"      - ãã®ä»–: {other_skipped}ä»¶")
    print(f"   ğŸ†• æ–°è¦è¿½åŠ : {added}ä»¶ï¼ˆä¸Šé™: {MAX_ARTICLES_PER_RUN}ä»¶ã€PR TIMESæœ€å¤§{PR_TIMES_MAX}ä»¶ï¼‰")
    print(f"   ğŸ“‚ ã‚µã‚¤ãƒˆåˆ¥å†…è¨³:")
    for source, count in source_counts.items():
        print(f"      - {source}: {count}ä»¶")
    print(f"   â­ é‡è¦åº¦åˆ†å¸ƒ:")
    print(f"      - â­â­â­â­â­ (å¿…èª­): {score_stats[5]}ä»¶")
    print(f"      - â­â­â­â­â˜† (é‡è¦): {score_stats[4]}ä»¶")
    print(f"      - â­â­â­â˜†â˜† (å‚è€ƒ): {score_stats[3]}ä»¶")
    print(f"      - â­â­â˜†â˜†â˜† (è»½ã„): {score_stats[2]}ä»¶")
    print(f"      - â­â˜†â˜†â˜†â˜† (ä½ã„): {score_stats[1]}ä»¶")
    print(f"   ğŸ“ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç·è¨˜äº‹æ•°: {total_articles}ä»¶")
    print(f"   ğŸ”— ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ: {exporter.get_spreadsheet_url()}")
    print("=" * 70)
    
    logger.info("=" * 70)
    logger.info("å‡¦ç†å®Œäº†")
    logger.info(f"åé›†è¨˜äº‹æ•°: {len(filtered_articles)}ä»¶")
    logger.info(f"ã‚¹ã‚­ãƒƒãƒ—: {skipped}ä»¶")
    logger.info(f"æ–°è¦è¿½åŠ : {added}ä»¶")
    logger.info(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç·è¨˜äº‹æ•°: {total_articles}ä»¶")
    logger.info("=" * 70)


if __name__ == "__main__":
    main()

