#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹
"""

import re
import time
from abc import ABC, abstractmethod
from typing import Optional
import requests
from bs4 import BeautifulSoup


class BaseScraper(ABC):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    # ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã™ã‚‹
    SITE_NAME = "Base"
    BASE_URL = ""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8"
        })
    
    @abstractmethod
    def get_article_list(self, max_pages: int = 3) -> list[dict]:
        """è¨˜äº‹ä¸€è¦§ã‚’å–å¾—ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        pass
    
    @abstractmethod
    def get_article_content(self, url: str) -> Optional[dict]:
        """è¨˜äº‹ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        pass
    
    def _fetch_page(self, url: str) -> Optional[BeautifulSoup]:
        """ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©åˆ‡ã«å‡¦ç†
            # 1. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰å–å¾—
            # 2. apparent_encodingã§è‡ªå‹•æ¤œå‡º
            # 3. UTF-8ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã¦ä½¿ç”¨
            encoding = response.encoding
            if not encoding or encoding == 'ISO-8859-1':
                encoding = response.apparent_encoding or 'utf-8'
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
            try:
                content = response.content.decode(encoding)
            except (UnicodeDecodeError, LookupError):
                try:
                    content = response.content.decode('utf-8')
                except UnicodeDecodeError:
                    content = response.content.decode('shift_jis', errors='ignore')
            
            return BeautifulSoup(content, "html.parser")
        except requests.RequestException as e:
            print(f"  âš ï¸ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ ({self.SITE_NAME}): {e}")
            return None
    
    def _extract_date(self, soup: BeautifulSoup, patterns: list[str] = None) -> str:
        """æ—¥ä»˜ã‚’æŠ½å‡º"""
        if patterns is None:
            patterns = [
                r"(\d{4}/\d{1,2}/\d{1,2})",
                r"(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)",
                r"(\d{4}-\d{2}-\d{2})",
            ]
        
        page_text = soup.get_text()
        for pattern in patterns:
            match = re.search(pattern, page_text)
            if match:
                return match.group(1)
        return ""
    
    def scrape(self, max_pages: int = 3, max_articles: int = 30) -> list[dict]:
        """è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦è¿”ã™"""
        print(f"\nğŸ“° {self.SITE_NAME} ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ä¸­...")
        
        # è¨˜äº‹ä¸€è¦§ã‚’å–å¾—
        articles = self.get_article_list(max_pages)
        print(f"   {len(articles)}ä»¶ã®è¨˜äº‹ã‚’ç™ºè¦‹")
        
        # è¨˜äº‹è©³ç´°ã‚’å–å¾—
        detailed_articles = []
        for i, article in enumerate(articles[:max_articles]):
            print(f"  ğŸ“° [{i+1}/{min(len(articles), max_articles)}] {article.get('url', '')[:50]}...")
            detail = self.get_article_content(article["url"])
            if detail:
                detail["source"] = self.SITE_NAME
                detailed_articles.append(detail)
            time.sleep(0.5)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
        
        print(f"   âœ… {len(detailed_articles)}ä»¶ã®è¨˜äº‹è©³ç´°ã‚’å–å¾—")
        return detailed_articles

